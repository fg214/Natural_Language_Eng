{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLE Assessed Coursework 1\n",
    "\n",
    "For this assessment, you are expected to complete and submit this notebook file.  When answers require code, you may import and use library functions (unless explicitly told otherwise).  All of your own code should be included in the notebook rather than imported from elsewhere.  Written answers should also be included in the notebook.  You should insert as many extra cells as you want and change the type between code and markdown as appropriate.\n",
    "\n",
    "In order to avoid misconduct, you should not talk about these coursework questions with your peers.  If you are not sure what a question is asking you to do or have any other questions, please ask me or one of the Teaching Assistants.\n",
    "\n",
    "Marking guidelines are provided as a separate document.\n",
    "\n",
    "In order to provide unique datasets for analysis by different students, you must enter your candidate number in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidateno=198934 #this MUST be updated to your candidate number so that you get a unique data sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preliminary imports\n",
    "import sys\n",
    "sys.path.append(r'\\\\ad.susx.ac.uk\\ITS\\TeachingResources\\Departments\\Informatics\\LanguageEngineering\\resources')\n",
    "sys.path.append(r'/Users/farahgee/Desktop/resources')\n",
    "sys.path.append(r'/Users/farahgee/Desktop/Week4Labs')\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from itertools import zip_longest\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.classify import accuracy \n",
    "from week3 import *\n",
    "import nltk\n",
    "from sussex_nltk.corpus_readers import AmazonReviewCorpusReader\n",
    "import random\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.classify import NaiveBayesClassifier \n",
    "\n",
    "from itertools import zip_longest\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: Naive Bayes Classification (25 marks)\n",
    "\n",
    "In this question, you will be considering how a Naive Bayes classifier can be applied to the task of deciding whether sentences are relevant or not relevant to the kitchen domain.\n",
    "\n",
    "The code below will generate for you two small unique sets of sentences, which you should refer to in your answer to this question.   This question will be marked on the quality of your explanations rather than the quality of your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences is 8.  Number of testing sentences is 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[({'we': True, 'use': True, 'least': True, 'day': True, 'kitchen': True},\n",
       "  'kitchen'),\n",
       " ({'the': True,\n",
       "   'wall': True,\n",
       "   'mount': True,\n",
       "   'bracket': True,\n",
       "   'causes': True,\n",
       "   'base': True,\n",
       "   'stick': True,\n",
       "   'straight': True,\n",
       "   'previous': True,\n",
       "   'reviewer': True,\n",
       "   'mentioned': True,\n",
       "   'usually': True,\n",
       "   'nose': True,\n",
       "   'given': True,\n",
       "   'location': True,\n",
       "   'mounts': True},\n",
       "  'not-kitchen'),\n",
       " ({'exellent': True,\n",
       "   'story': True,\n",
       "   'actors': True,\n",
       "   'great': True,\n",
       "   'buy': True,\n",
       "   'watch': True,\n",
       "   'show': True,\n",
       "   'family': True},\n",
       "  'not-kitchen'),\n",
       " ({'while': True,\n",
       "   'dvd': True,\n",
       "   'piles': True,\n",
       "   'extras': True,\n",
       "   'making': True,\n",
       "   'retrospective': True,\n",
       "   'entertaining': True,\n",
       "   'informative': True,\n",
       "   'film': True,\n",
       "   'attraction': True},\n",
       "  'not-kitchen'),\n",
       " ({'the': True,\n",
       "   'gps': True,\n",
       "   'good': True,\n",
       "   'great': True,\n",
       "   'i': True,\n",
       "   'must': True,\n",
       "   'go': True,\n",
       "   'outdoors': True,\n",
       "   'pick': True,\n",
       "   'signal': True,\n",
       "   'even': True,\n",
       "   'takes': True,\n",
       "   'minutes': True,\n",
       "   'get': True},\n",
       "  'not-kitchen'),\n",
       " ({'save': True, 'money': True, 'try': True, 'different': True, 'brand': True},\n",
       "  'kitchen'),\n",
       " ({'although': True,\n",
       "   'dead': True,\n",
       "   'soul': True,\n",
       "   'begins': True,\n",
       "   'quite': True,\n",
       "   'decently': True,\n",
       "   'surprisingly': True,\n",
       "   'doss': True,\n",
       "   'tin': True,\n",
       "   'ear': True,\n",
       "   'dialogue': True,\n",
       "   'dearly': True,\n",
       "   'loves': True,\n",
       "   'cardboard': True,\n",
       "   'character': True,\n",
       "   'sense': True,\n",
       "   'plot': True,\n",
       "   'rapidly': True,\n",
       "   'unsurprisingly': True,\n",
       "   'disintegrates': True,\n",
       "   'mangle': True,\n",
       "   'missed': True,\n",
       "   'opportunities': True},\n",
       "  'not-kitchen'),\n",
       " ({'ishikawa': True,\n",
       "   'makes': True,\n",
       "   'careful': True,\n",
       "   'intense': True,\n",
       "   'analysis': True,\n",
       "   'meaning': True,\n",
       "   'quality': True,\n",
       "   'control': True},\n",
       "  'not-kitchen')]"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Do NOT change the code in this cell.\n",
    "\n",
    "topics=[\"book\",\"kitchen\",\"dvd\",\"electronics\"]\n",
    "samplesize=3\n",
    "stop=stopwords.words('english')\n",
    "\n",
    "trainingsentences=[]\n",
    "testsentences=[]\n",
    "cr = AmazonReviewCorpusReader()\n",
    "\n",
    "for topic in topics:\n",
    "    random.seed(candidateno)\n",
    "    if topic == \"kitchen\":\n",
    "        key=\"kitchen\"\n",
    "    else:\n",
    "        key=\"not-kitchen\"\n",
    "    topicsentences=[]\n",
    "    while len(topicsentences)<2:\n",
    "        topicsentences=[({token.lower():True for token in doc if token not in stop and token.isalpha()},key) for doc in cr.category(topic).sample_sents(samplesize=samplesize) if len(doc)>0]\n",
    "    testsentences.append(topicsentences[0])\n",
    "    trainingsentences+=topicsentences[1:]\n",
    "\n",
    "random.shuffle(trainingsentences)\n",
    "random.shuffle(testsentences)\n",
    "print(\"Number of training sentences is {}.  Number of testing sentences is {}\".format(len(trainingsentences),len(testsentences)))\n",
    "trainingsentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) With reference to the sentences generated above, **explain** how a Naive Bayes classifer would be trained to carry out the task of deciding whether sentences are relevant to the kitchen domain.  You do **not** need to build or train a classifier.  However, you should explain the relevant probabilities with reference to examples taken from your samples of sentences.  \\[10 marks\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#1) a naive bayes classifier uses  supervised training which splits the document into training and testing sets.\n",
    "#2) we generate a wordlist from the document of labelled words,  when we generate a wordlist it is used to create the model of relevant wor  lists, which is then used to decide whether a word is relevant or not\n",
    "#3) the training set is used on the list of labelled words, hence 'supervised learning', based on the percen\n",
    "\n",
    "#1) the probability of the sentence being in the kitchen (either in kitchen or not kitchen)\n",
    "\n",
    "\n",
    "A classifier consults a model which has been given to it or learnt in someway, and then decides which of the following classes is the most likely class. \n",
    "This model comes from learnt examples, in this case the Naive Bayes, which can take the examples and turn that into a set of weights or parameters from \n",
    "which it can then make a decision for unseen docuements. the classifier encoder then takes the labelled data and will produce a model (which is a list or tuple of parameters), \n",
    "it will take that model and some unlabelled data, and give it to the classifier decoder which will then be able to make a decision for each document in the unlabelled data to \n",
    "decide what the label should be. so at 'decode time', we are just taking in a set of documents, with no labels attached to the feature representation, and we produce the data \n",
    "to the same format we see the labelled data in. \n",
    "\n",
    "\n",
    "The first step a naive bayes classifier goes through in order to classify a document, d, is to to determine which of these probabilities is greatest:\n",
    "        \n",
    "        p(kitchen|d)  versus P(not-kitchen|d)\n",
    "d could, for example, be the string \"i bought coffee maker match kitchenaid blender gridner kitchen\" \n",
    "P(kicthen|\"i bought coffee maker match kitchenaid blender gridner kitchen\") \n",
    "versus P(not-kitchen|\"i bought coffee maker match kitchenaid blender gridner kitchen\")\n",
    "the idea is that if the value on of p(kitchen|d) is higher then the document is in the \n",
    "category domain kitchen, otherwise if the the probability p(not-kitchen|d) is higher \n",
    "then the document is in the catergory not-kitchen\n",
    "\n",
    "\n",
    "P(F|G) means the probability of F given G. \n",
    "So, P(kitchen|d) means the probability, \n",
    "given a document d, where d is of class Kitchen.\n",
    "\n",
    "we are going to use something called Bayes' rule which states that:\n",
    "    \n",
    "        P(F|G) = P(G|F).P(F)/P(G)\n",
    "        \n",
    "Applying Bayes rule to our problem leads to the following comparison: \n",
    "\n",
    "P(kitchen|d) = P(d|kitchen).P(kitchen)/p(d)  versus  p(d|not-kitchen).p(not-kitchen)/p(d)\n",
    "\n",
    "since both sides are being divided by the same thing, we only need to make the following comparison:\n",
    "    \n",
    "        p(d|kitchen).p(kitchen)  versus p(d|not-kitchen).p(not-kitchen)\n",
    "        \n",
    "the following is an explanation of what each of these probabilities mean\n",
    "\n",
    "1. P(d|kitchen): this is the probability of a document in the kitchen domain being the document d\n",
    "2. P(d|not-kitchen): this is the probability of a document in the not-kitchen domain being the document d\n",
    "3. P(kitchen): this is the probability of a random selected document being of category kitchen \n",
    "4. p(not-kitchen): this is the probability of a randomly selected document of catrgory not-kitchen \n",
    "            \n",
    "to obtain these probabilities, we go into the classifier encode phase, which learns the model that will be used. \n",
    "we must acknowledge the class priors, P(kitchen) and P(not-kitchen). \n",
    "the following is the calculation for the class priors: \n",
    "since we have 3 training documents which are under the kitchen domain, and 5 traning documents which are under the \n",
    "not-kitchen domain.\n",
    "\n",
    "        P(kitchen)= 3/8 = 0.375   p(not-kitchen) = 5/8 = 0.625\n",
    "P(kicthen|\"i bought coffee maker match kitchenaid blender gridner kitchen\")\n",
    "\n",
    "we must obtain the estimated values for these conditional probabilities from our training data\n",
    "\n",
    "p(\"i\"|kitchen ) = 0/10\n",
    "p(\"bought\"|kitchen|) = 0/10\n",
    "p(\"coffee\"|kitchen) = 0/10\n",
    "p(\"maker\"|kitchen)= 0/10\n",
    "p(\"match\"|kithen) = 0/10\n",
    "p(\"kitchenaid\"|kitchen) = 0/10\n",
    "p(\"blender\"|kitchen) = 0/10\n",
    "p(\"grinder\"|kitchen) =0/10\n",
    "p(\"kitchen\"|kitchen) = 1/10\n",
    "\n",
    "Since most of the tokens that are in our test sentence do not appear in our training documents, \n",
    "one or more zeros in the probabilities leads to  data sparsness once the probabilities are mulitiplied \n",
    "by each other. \n",
    "to avoid this, we apply smoothing, which adds-one, to each of the values applied, that way if a feature has not been\n",
    "seen, we give it a count of 1. if it has been seen once, we will give it a count of 2, and so on. which means that low \n",
    "probability events get low probabilities do not get 0's. \n",
    "#applying SMOOTHING for kitchen \n",
    "p(\"i\"|kitchen ) = 1/10\n",
    "p(\"bought\"|kitchen|) = 1/10\n",
    "p(\"coffee\"|kitchen) = 1/10\n",
    "p(\"maker\"|kitchen)= 1/10\n",
    "p(\"match\"|kithen) = 1/10\n",
    "p(\"kitchenaid\"|kitchen) = 1/10\n",
    "p(\"blender\"|kitchen) = 1/10\n",
    "p(\"grinder\"|kitchen) =1/10\n",
    "p(\"kitchen\"|kitchen) = 2/10\n",
    "\n",
    "#calculate the probability by multiplying all individual probabilities\n",
    "p(\"i bought coffee maker match kitchenaid blender gridner kitchen\"|kitchen) = \n",
    "p(\"i\"|kitchen) x p(\"bought\"|kitchen|) x p(\"coffee\"|kitchen) x p(\"maker\"|kitchen) x p(\"match\"|kithen) x p(\"kitchenaid\"|kitchen) x p(\"blender\"|kitchen) x p(\"grinder\"|kitchen) x p(\"kitchen\"|kitchen)\n",
    "p(\"i bought coffee maker match kitchenaid blender gridner kitchen\"|kitchen) = 0.16 = 16%\n",
    "\n",
    "#now calculate the probability of a document, d, given not kitchen \n",
    "p(not-kitchen|\"i bought coffee maker match kitchenaid blender gridner kitchen\")\n",
    "\n",
    "p(\"i\"|not-kitchen ) = 0/78 \n",
    "p(\"bought\"|not-kitchen) = 0/78\n",
    "p(\"coffee\"|not-kitchen) = 0/78\n",
    "p(\"maker\"|not-kitchen)= 0/78\n",
    "p(\"match\"|not-kithen) = 0/78\n",
    "p(\"kitchenaid\"|not-kitchen) = 0/78\n",
    "p(\"blender\"|not-kitchen) = 0/78\n",
    "p(\"grinder\"|not-kitchen) = 0/78\n",
    "p(\"kitchen\"|not-kitchen) = 0/78\n",
    "\n",
    "\n",
    "smoothing will be applied to the not-kitchen domain as well, due to an occurence of a 0 in one or more of the features' probabilities.\n",
    "#applying SMOOTHING for not-kitchen \n",
    "p(\"i\"|not-kitchen ) = 1/78 \n",
    "p(\"bought\"|not-kitchen) = 1/78\n",
    "p(\"coffee\"|not-kitchen) = 1/78\n",
    "p(\"maker\"|not-kitchen)= 1/78\n",
    "p(\"match\"|not-kithen) = 1/78\n",
    "p(\"kitchenaid\"|not-kitchen) = 1/78\n",
    "p(\"blender\"|not-kitchen) = 1/78\n",
    "p(\"grinder\"|not-kitchen) = 1/78\n",
    "p(\"kitchen\"|not-kitchen) = 1/78\n",
    "\n",
    "p(\"i bought coffee maker match kitchenaid blender gridner kitchen\"|not-kitchen) = p(\"i\"|not-kitchen) x p(\"bought\"|not-kitchen|) x p(\"coffee\"|not-kitchen) x p(\"maker\"|not-kitchen) x p(\"match\"|not-kithen) x p(\"kitchenaid\"|not-kitchen) x p(\"blender\"|not-kitchen) x p(\"grinder\"|not-kitchen) x p(\"kitchen\"|not-kitchen)\n",
    "p(\"i bought coffee maker match kitchenaid blender gridner kitchen\"|not-kitchen) = 0.115 = 11.5%\n",
    "\n",
    "As we can see p(kitchen|\"i bought coffee maker match kitchenaid blender gridner kitchen\")> p(not-kithcen|\"i bought coffee maker match kitchenaid blender gridner kitchen\")\n",
    "meaning the bag-of-words belongs to class kitchen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) With reference to the sentences generated above, **explain** how a trained Naive Bayes classifier would assign a class to a sentence. \\[5 marks\\]\n",
    "\n",
    "the naive bayes classifier \n",
    "#explain what the classifier does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#calculate the probabilities above \n",
    "When the  feature extractor turms a document into a set or vector of features\n",
    "the classfier then a consults a model of what features to expect in different classes and \n",
    "decides the most likely accordingly. this model is learnt from the naive bayes method.\n",
    "\n",
    "the classifier comes up with a probability of whether the document is in class Kitchen or in class not-kitchen,\n",
    "using Bayes Law, which was shown above.\n",
    "\n",
    "P(F|G) means the probability of F given G. \n",
    "So, P(kitchen|d) means the probability, \n",
    "given a document d, where d is of class Kitchen.\n",
    "\n",
    "P(kitchen)= 3/8 = 0.375   p(not-kitchen) = 5/8 = 0.625\n",
    "P(kicthen|\"i bought coffee maker match kitchenaid blender gridner kitchen\")\n",
    "\n",
    "the conditional probability is calculated \n",
    "p(\"i\"|kitchen ) = 0/10\n",
    "p(\"bought\"|kitchen|) = 0/10\n",
    "p(\"coffee\"|kitchen) = 0/10\n",
    "p(\"maker\"|kitchen)= 0/10\n",
    "p(\"match\"|kithen) = 0/10\n",
    "p(\"kitchenaid\"|kitchen) = 0/10\n",
    "p(\"blender\"|kitchen) = 0/10\n",
    "p(\"grinder\"|kitchen) =0/10\n",
    "p(\"kitchen\"|kitchen) = 1/10\n",
    "\n",
    "after smoothing is applied:\n",
    "\n",
    "p(\"i\"|kitchen ) = 1/10\n",
    "p(\"bought\"|kitchen|) = 1/10\n",
    "p(\"coffee\"|kitchen) = 1/10\n",
    "p(\"maker\"|kitchen)= 1/10\n",
    "p(\"match\"|kithen) = 1/10\n",
    "p(\"kitchenaid\"|kitchen) = 1/10\n",
    "p(\"blender\"|kitchen) = 1/10\n",
    "p(\"grinder\"|kitchen) =1/10\n",
    "p(\"kitchen\"|kitchen) = 2/10\n",
    "\n",
    "the classifier uses these probabilities to determine whether a document is of a specific class, in this case Kitchen.\n",
    "this is done by calculating the probability of each individual word occuring in the predicted class, using conditional\n",
    "probabilitiy. The example shown above for the bag of words \"i bought coffee maker match kitchenaid blender gridner kitchen\",\n",
    "first the class priori is calculated which is the probability of kitchen, 0.375. \n",
    "\n",
    "the conditional probabilities are calculated by finding the inidividual probability of each word in that sentence to be \n",
    "of class kitchen. Since most of the tokens that are in our test sentence do not appear in our training documents, \n",
    "one or more zeros in the probabilities may appear and lead to  data sparsness once the probabilities are mulitiplied \n",
    "by each other. \n",
    "\n",
    "therefore 'smoothing' is applied to these probabilities. smoothing is adding one to the counter to each of the values applied, that way if a feature has not been\n",
    "seen, we give it a count of 1. if it has been seen once, we will give it a count of 2, and so on. which means that low \n",
    "probability events get low probabilities do not get 0's. \n",
    "\n",
    "finally, all the conditional probabilities are multiplied by each other and multiplied by the prior probability.\n",
    "\n",
    "p(\"i bought coffee maker match kitchenaid blender gridner kitchen\"|kitchen) = \n",
    "p(\"i\"|kitchen) x p(\"bought\"|kitchen|) x p(\"coffee\"|kitchen) x p(\"maker\"|kitchen) x p(\"match\"|kithen) x p(\"kitchenaid\"|kitchen) x p(\"blender\"|kitchen) x p(\"grinder\"|kitchen) x p(\"kitchen\"|kitchen)\n",
    "p(\"i bought coffee maker match kitchenaid blender gridner kitchen\"|kitchen) = 0.16 = 16%\n",
    "\n",
    "This percentage is higher than that of the (p|not-kitchen), which was 0.115%, meaning that the classifier identified the bag of words as the domain kitchen.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) The labels assigned to the sentences contained in the variable `testsentences` above are \\['kitchen','not-kitchen','not-kitchen','not-kitchen'\\].  Using this example to illustrate your answer, **explain** how each of the performance metrics of *accuracy*, *precision*, *recall* and *F1-score* are calculated.  Which of these metrics would you use to choose between classification models in this example?  **Justify your answer**. \\[10 marks\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#q1) explain how each of  the performance metrics\n",
    "#of a) accuracy, b) precision, c) recall and d) F1-score are calculated.\n",
    "#\n",
    "#q2)Which of these metrics would you use to choose \n",
    "# between classification models in this example?\n",
    "\n",
    "q1a)\n",
    "    In order to evaluate a classifier, we an look at the acccuracy and error rate.\n",
    "accuracy is the proportion of items in the test set that are classified correctly, it checks \n",
    "for every single document in our set, if the prediction matched the label.\n",
    "This is demosntrated on a 'confusion matrix', where the matrix is made up of predicted classes x \n",
    "true classes; the predictions we got right go on the leading diagonal. \n",
    "    The equation to calculate the accuracy is by: adding the true positives, or TP,(values we \n",
    "predicted as positive, and were actually positive) to the TN, or true negative(the classes we predicted as negative, \n",
    "and were actually negative) as numerators, and divide that by the value of N, which is the total number of data items.\n",
    "    In the case of the kitchen domain, this can be done easily in a table representation. The actual test sentences obtained \n",
    "['kitchen','not-kitchen','not-kitchen','not-kitchen'], and the actual sentences are \n",
    "['kitchen','not-kitchen','not-kitchen','not-kitchen'].  This means our confusion matrix will show TP for [kitchen] and [kitchen], and \n",
    "TN for the preceeding three [not-kitchen]'s. \n",
    "    so if we were to put this in thr accuracy equation, TP + TN/N, we would get: \n",
    "1+3/4 = 4/4 = 1 = 100% accuracy \n",
    "this shows that for every sentence in our set, the classifier was 100% accurate in predicting the class in which it belonged to.\n",
    "\n",
    "    1b)\n",
    "#precision \n",
    "In order to calculate the precision, we must know out of all the things that were predicted as postitive, how many are actually positive.\n",
    "true positive/true positive + false positive \n",
    "1/1+0 = 1/1 = 1 = 100% precision \n",
    "    1c)\n",
    "#recall\n",
    "recall is the proportion of actually positive documents that are predicted correctly, \n",
    "so we consider  all the things we could have predicted as positive, how many did we actually get. \n",
    "recall = TP/TP + FN, so if we apply this equation to our assigned sentences above,\n",
    "1/1+0 = 1/1 = 1 = 100% recall\n",
    "\n",
    "these values can be obtained from adding an additional total column and row to our existing confusion \n",
    "matrix. \n",
    "\n",
    "    1d) \n",
    "#f1score\n",
    "we want out classifiers to have high recision and high recall, so we can combine then into one equation called th f1 score.\n",
    "f1 = 2 x P x R /P + R,   (where p is precision, and r is recall)\n",
    "This formula uses harmonic mean, instead of arithmetic, is is a measure of central tendancy,\n",
    "however is is biased or weighted towards the lower value/number\n",
    "so in the case of kitchen domain,\n",
    "\n",
    "2 x 1 x 1/1 + 1 = 2/2 = 1 \n",
    "f1 score = 1 = 100%\n",
    "\n",
    "this shows to have a 'good' classifier, you need both the recall and the precision to be higher values,\n",
    "since a low value in either will bring down the overall f1 score.\n",
    "\n",
    "#pick a metric to use for naive bayes classifier and discuss why\n",
    "q2)\n",
    "    The most suitable metric to be chosen in this situatio, would be the recall.\n",
    "By elimination, using precision as a measurement isnt necesarrily helpful. \n",
    "very high precision could mean that the training class is too fitting or specific \n",
    "    \n",
    "#tradeoff \n",
    "However, there are occasions we would want to trade of either of them. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Training Data for Sentiment Analysis (25 marks)\n",
    "The objective of this question is to investigate the extent to which performance of a Naive Bayes classifier is affected by the quantity and quality of the training data.  Does more training data mean better performance?  Is performance degraded if we train on one domain and test on another domain?  For example, suppose we train a sentiment classifier on book reviews and then test that classifier on a collection of dvd reviews. Does it perform as well as it would when trained on dvd reviews?\n",
    "\n",
    "The code below is included to enable you to get pre-formatted training and test data for a given category (evenly split between positive and negative reviews).  In this question, there are marks available for the quality of your programming, your experimental design and your interpretation of results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, ratio=0.7): # when the second argument is not given, it defaults to 0.7\n",
    "    \"\"\"\n",
    "    Given corpus generator and ratio:\n",
    "     - partitions the corpus into training data and test data, where the proportion in train is ratio,\n",
    "\n",
    "    :param data: A corpus generator.\n",
    "    :param ratio: The proportion of training documents (default 0.7)\n",
    "    :return: a pair (tuple) of lists where the first element of the \n",
    "            pair is a list of the training data and the second is a list of the test data.\n",
    "    \"\"\"\n",
    "    \n",
    "    data = list(data)  \n",
    "    n = len(data)  \n",
    "    train_indices = random.sample(range(n), int(n * ratio))          \n",
    "    test_indices = list(set(range(n)) - set(train_indices))    \n",
    "    train = [data[i] for i in train_indices]           \n",
    "    test = [data[i] for i in test_indices]             \n",
    "    return (train, test)                       \n",
    " \n",
    "\n",
    "def feature_extract(review):\n",
    "    \"\"\"\n",
    "    Generate a feature representation for a review\n",
    "    :param review: AmazonReview object\n",
    "    :return: dictionary of Boolean features\n",
    "    \"\"\"\n",
    "    return {word:True for word in review.words()}\n",
    "\n",
    "def get_training_test_data(category,ratio=0.7,seed=candidateno):\n",
    "    \"\"\"\n",
    "    Get training and test data for a given category and ratio, pre-formatted for use with NB classifier\n",
    "    :param category: category of review corpus, one of [\"kitchen, \"dvd, \"book\", \"electronics\"]\n",
    "    :param ratio: proportion of data to use as training data\n",
    "    :return: pair of lists \n",
    "    \"\"\"\n",
    "    reader=AmazonReviewCorpusReader().category(category)\n",
    "    random.seed(candidateno)\n",
    "    pos_train, pos_test = split_data(reader.positive().documents(),ratio=ratio)\n",
    "    neg_train, neg_test = split_data(reader.negative().documents(),ratio=ratio)\n",
    "    train_data=[(feature_extract(review),'P')for review in pos_train]+[(feature_extract(review),'N') for review in neg_train]\n",
    "    test_data=[(feature_extract(review),'P')for review in pos_test]+[(feature_extract(review),'N') for review in neg_test]\n",
    "    return train_data,test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) By varying the amount of training data used, **investigate** the impact of the amount of training data used on the accuracy of a Naive Bayes classifier for each of the four domains: *dvd*, *book*, *kitchen* and *electronics*.  You should use the NaiveBayesClassifier from the `nltk.classify` library.  You should also use a table and an appropriate graph(s) to display your results.  Make sure you **discuss** your results and conclusions. \\[8 marks\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_evaluate(category, r):\n",
    "    training, testing = get_training_test_data(category, r)\n",
    "    classifier = NaiveBayesClassifier.train(training)\n",
    "    return accuracy(classifier, testing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R0.2</th>\n",
       "      <th>R0.4</th>\n",
       "      <th>R0.5</th>\n",
       "      <th>R0.8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>book</th>\n",
       "      <td>0.683750</td>\n",
       "      <td>0.729167</td>\n",
       "      <td>0.734</td>\n",
       "      <td>0.785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kitchen</th>\n",
       "      <td>0.780625</td>\n",
       "      <td>0.800833</td>\n",
       "      <td>0.793</td>\n",
       "      <td>0.830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dvd</th>\n",
       "      <td>0.645000</td>\n",
       "      <td>0.678333</td>\n",
       "      <td>0.723</td>\n",
       "      <td>0.775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>electronics</th>\n",
       "      <td>0.658750</td>\n",
       "      <td>0.752500</td>\n",
       "      <td>0.769</td>\n",
       "      <td>0.770</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 R0.2      R0.4   R0.5   R0.8\n",
       "book         0.683750  0.729167  0.734  0.785\n",
       "kitchen      0.780625  0.800833  0.793  0.830\n",
       "dvd          0.645000  0.678333  0.723  0.775\n",
       "electronics  0.658750  0.752500  0.769  0.770"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r02 = [classifier_evaluate(\"book\",0.2), classifier_evaluate(\"kitchen\", 0.2),  classifier_evaluate(\"dvd\", 0.2), classifier_evaluate(\"electronics\", 0.2)]\n",
    "r04 = [classifier_evaluate(\"book\",0.4), classifier_evaluate(\"kitchen\", 0.4),  classifier_evaluate(\"dvd\", 0.4), classifier_evaluate(\"electronics\", 0.4)]\n",
    "r05 = [classifier_evaluate(\"book\",0.5), classifier_evaluate(\"kitchen\", 0.5),  classifier_evaluate(\"dvd\", 0.5), classifier_evaluate(\"electronics\", 0.5)]\n",
    "r08 = [classifier_evaluate(\"book\",0.8), classifier_evaluate(\"kitchen\", 0.8),  classifier_evaluate(\"dvd\", 0.8), classifier_evaluate(\"electronics\", 0.8)]\n",
    "\n",
    "pd.DataFrame(list(zip_longest(r02,r04,r05,r08)),columns=[\"R0.2\", \"R0.4\",\"R0.5\", \"R0.8\"], index= [\"book\", \"kitchen\", \"dvd\", \"electronics\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#x is always constant \n",
    "#topics=[\"book\",\"kitchen\",\"dvd\",\"electronics\"]\n",
    "x = [0.2, 0.4, 0.5, 0.8]\n",
    "y1 = [classifier_evaluate(\"book\", 0.2),classifier_evaluate(\"book\", 0.4),classifier_evaluate(\"book\", 0.5), classifier_evaluate(\"book\", 0.8)]\n",
    "plt.plot(x, y1, label = \"book\")\n",
    "\n",
    "y2 = [classifier_evaluate(\"kitchen\", 0.2),classifier_evaluate(\"kitchen\", 0.4),classifier_evaluate(\"kitchen\", 0.5), classifier_evaluate(\"kitchen\", 0.8)]\n",
    "plt.plot (x, y2, label = \"kitchen\")\n",
    "\n",
    "y3 = [classifier_evaluate(\"dvd\", 0.2),classifier_evaluate(\"dvd\", 0.4),classifier_evaluate(\"dvd\", 0.5), classifier_evaluate(\"dvd\", 0.8)]\n",
    "plt.plot (x , y3, label = \"dvd\")\n",
    "\n",
    "y4 = [classifier_evaluate(\"electronics\", 0.2),classifier_evaluate(\"electronics\", 0.4),classifier_evaluate(\"electronics\", 0.5), classifier_evaluate(\"electronics\", 0.8)]\n",
    "plt.plot (x, y4, label = \"electronics\")\n",
    "\n",
    "plt.xlabel('ratio/r')\n",
    "plt.ylabel('accuracy')\n",
    "\n",
    "#shows the key table\n",
    "plt.legend()\n",
    "#shows the graph\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table and graph shown above suggest that the lower the training data the lower the accuracy is, \n",
    "and conversely, the higher the training data, the higher the accuracy is.\n",
    "\n",
    "for instance if we look at the kicthen domain as our example, we can see that when the ratio of training to testing\n",
    "is 0.2, the accuracy is 0.6, and when the ratio is 0.4, the accuracy increases to 0.729, and increases again for the ratio of 0.5 to 0.79.\n",
    "finally the accuracy is the highest at the highest training ratio, where 80% of the documents are being used for training and 20% for testing.\n",
    "\n",
    "this pattern is present for the remaining domains. this suggests that the quanitity of data used for training affects the classifier, and in turn \n",
    "trains it better to become more accurate in its predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) For each possible combination of source and target domain, **evaluate** the accuracy of a Naive Bayes classifier trained on the source domain and tested on the target domain.  There are four domains so there are 16 possible combinations you should consider.  You should use a table and an appropriate graph(s) to display your results.  Make sure you **discuss** your results and conclusions. \\[8 marks\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combinations(category1, category2, r):\n",
    "    training1, testing1 = get_training_test_data(category1, r)\n",
    "    training2, testing2 = get_training_test_data(category2, r)\n",
    "    classifier = NaiveBayesClassifier.train(training1)\n",
    "    return accuracy(classifier, testing2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book</th>\n",
       "      <th>kitchen</th>\n",
       "      <th>dvd</th>\n",
       "      <th>electronics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>book</th>\n",
       "      <td>0.7850</td>\n",
       "      <td>0.6600</td>\n",
       "      <td>0.7175</td>\n",
       "      <td>0.6525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kitchen</th>\n",
       "      <td>0.6150</td>\n",
       "      <td>0.8300</td>\n",
       "      <td>0.6750</td>\n",
       "      <td>0.7625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dvd</th>\n",
       "      <td>0.7100</td>\n",
       "      <td>0.6525</td>\n",
       "      <td>0.7750</td>\n",
       "      <td>0.6650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>electronics</th>\n",
       "      <td>0.6275</td>\n",
       "      <td>0.7850</td>\n",
       "      <td>0.6925</td>\n",
       "      <td>0.7700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               book  kitchen     dvd  electronics\n",
       "book         0.7850   0.6600  0.7175       0.6525\n",
       "kitchen      0.6150   0.8300  0.6750       0.7625\n",
       "dvd          0.7100   0.6525  0.7750       0.6650\n",
       "electronics  0.6275   0.7850  0.6925       0.7700"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posofbook = [combinations(\"book\", \"book\", 0.8), combinations(\"book\",\"kitchen\", 0.8), combinations(\"book\", \"dvd\", 0.8), combinations(\"book\", \"electronics\", 0.8)]\n",
    "posofkitchen =[combinations(\"kitchen\", \"book\", 0.8), combinations(\"kitchen\", \"kitchen\", 0.8), combinations(\"kitchen\",\"dvd\",0.8), combinations(\"kitchen\", \"electronics\", 0.8)]\n",
    "posofdvd=[combinations(\"dvd\",\"book\",0.8), combinations(\"dvd\", \"kitchen\", 0.8), combinations(\"dvd\", \"dvd\", 0.8), combinations(\"dvd\", \"electronics\", 0.8)]\n",
    "posofelectronics = [combinations(\"electronics\", \"book\", 0.8), combinations(\"electronics\", \"kitchen\", 0.8), combinations(\"electronics\", \"dvd\", 0.8), combinations(\"electronics\", \"electronics\", 0.8)]\n",
    "\n",
    "\n",
    "pd.DataFrame(list(zip_longest(posofbook,posofkitchen,posofdvd,posofelectronics)), columns=[\"book\", \"kitchen\", \"dvd\", \"electronics\"],index=[\"book\", \"kitchen\", \"dvd\", \"electronics\"])\n",
    "\n",
    "#now you have just tried showing the accuracy of the training data of x on the test set of y, \n",
    "#the following is the accuracy table for each of those combinations of combination 1 and 2.\n",
    "#now make a table to represent this nicely\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3XucVXW9//HX20EYvKIwmgoEGqkkiTFgmimVGdZR0ryA90r5cUrTSku7GPorj5Y/O+eEVngJT2mAqYlIgGGKJhqDggrKCVFjBGPAu3H38/tjrcHNds/MZpjFrJl5Px+P/WBdvmutz1rs2Z/9/a61v19FBGZmZnmzXWsHYGZmVooTlJmZ5ZITlJmZ5ZITlJmZ5ZITlJmZ5ZITlJmZ5ZITlHVIkj4paVHB/IuSjm6g7FBJtdsuui0jqY+kkNSpgfXfk3RTRsfe7DqataSSb2iz9i4iHgb2b+04toWIuKql9iUpgH4RsTjdd4e5jrbtuQZlZma55ARluZE2s10maaGk1yT9RlJlwfrzJC2W9KqkyZL2TpdL0s8lrZD0hqSnJB2Urvt8ur+3JL0s6eJ0ealmu8ENHbsozr0l3SmpTtILkr7RyDl1lfT/JL2UxvaIpK7puuMlLZD0uqQHJR1YdC0uSc/lHUk3S9pT0p/Sc/mzpN2KDvcVScskLZf07YJ9jZH0u3S6vjnwbEn/kLRS0vcLyg6RNDuNabmksZI6p+tmpcXmS3pb0qnF11HSgem5vJ6e2/EF68ZLul7Sfek5PC5pv6b+D60Diwi//MrFC3gReAboBewO/BX4cbru08BK4GNAF+AXwKx03eeAuUA3QMCBwF7puuXAJ9Pp3YCPpdNDgdoyj72pLMmXurnA5UBnYF9gCfC5Bs7peuBBYB+gAjg8jf/DwDvAZ4Htge8Ai4HOBfE8BuyZbrsCeAI4JN3+AeBHadk+QAC/B3YEBgB1wNHp+jHA74rK3gh0BQ4G1gIHpusHAR8naf7vAzwLXFRwPgF8qGC+8Npsn57D99Jr82ngLWD/dP144FVgSLr/24AJTf0f+tVxX65BWd6MjYilEfEq8BNgZLr8dOCWiHgiItYClwGHSeoDrAd2Bg4AFBHPRsTydLv1QH9Ju0TEaxHxRDOOXWgwUBURV0bEuohYQvJhP6K4oKTtgK8AF0bEyxGxMSIeTeM/FbgvIu6PiPXAtSQJ4/CCXfwiIv4ZES8DDwOPR8ST6fZ3kySrQldExDsR8TTwmwbiLyy7OiLmA/NJEhURMTciHouIDRHxIvBr4KhG9lPo48BOwNXptXkAmFIUx10R8beI2ECSoAamyxv7P7QOygnK8mZpwfRLwN7p9N7pPAAR8TawCtgn/SAcS1Jb+aekcZJ2SYt+Cfg88JKkhyQd1oxjF/ogsHfahPW6pNdJagx7lijbA6gEni+xrvh83k2Pv09BmX8WTK8uMb9TM+Kv90rB9L/q9yXpw5KmSHpF0pvAVel5lGNvYGl6LoVxFJ5TyeM28X9oHZQTlOVNr4Lp3sCydHoZSXIAQNKOQHfgZYCI+O+IGAR8hKT57JJ0+ZyIGA7sAfwRmNSMYxdaCrwQEd0KXjtHxOdLlF0JrAH2K7Gu+HyUHv/lRuJrSjnxN+WXwHMkT+rtQpJ8Vea2y4Beac2xMI6yzqmh/0PruJygLG++LqmnpN1JPhwnpstvB74saaCkLiTf7B+PiBclDZZ0qKTtSe7rrAE2Suos6XRJu6bNaG8CG5tx7EJ/A96U9N30AYgKSQdJGlxcMK1J3AJclz5YUSHpsDT+ScAXJH0mjfvbJPeCHt3iK/aeH0raQdJHgC83EH9Tdia5Tm9LOgD496L1/yS571bK4yTX/zuStpc0FDgOmNDUQRv6P2xG/NaOOEFZ3twOzCB58GAJ8GOAiJgJ/BC4k+TBh/14777PLiT3gV4jaVJaRXJPB+BM4MW0uWo0cMaWHrtQRGwk+dAdCLxAUku6Cdi1gX1eDDwNzCF5QOAaYLuIWJTG8ot0H8cBx0XEukbia8pDJA8pzASujYgZzdjHxcBpJA833Mj7k9wY4Na0efOUwhVp7McDx5Kc0w3AWRHxXBnHbez/0DooRXjAQssHSS8C50bEn1s7FjNrfa5BmZlZLjlBmZlZLrmJz8zMcsk1KDMzy6U215t5jx49ok+fPq0dhpmZNdPcuXNXRkRVU+XaXILq06cPNTU1rR2GmZk1k6SXmi7lJj4zM8spJygzM8slJygzM8ulNncPyswsD9avX09tbS1r1qxp7VByq7Kykp49e7L99ts3a3snKDOzZqitrWXnnXemT58+JJ3RW6GIYNWqVdTW1tK3b99m7cNNfGZmzbBmzRq6d+/u5NQASXTv3n2raphOUGZmzeTk1LitvT5OUGZmlku+B2Vm1gKqq1t2f+X0R1BRUcGAAQOICCoqKhg7diyHH374Fh/rwQcf5Nprr2XKlCnNiDQ7TlCWa9XjmvdXXzPKvY1Y+9e1a1fmzZsHwPTp07nssst46KGHWjmqluMmPjOzduDNN99kt912A5In6C655BIOOuggBgwYwMSJExtdXmjOnDkccsghLFmyZJvGX4prUGZmbdTq1asZOHAga9asYfny5TzwwAMA3HXXXcybN4/58+ezcuVKBg8ezJFHHsmjjz5acnm9Rx99lAsuuIB77rmH3r17t9ZpbeIEZWbWRhU28c2ePZuzzjqLZ555hkceeYSRI0dSUVHBnnvuyVFHHcWcOXMaXL7LLrvw7LPPMmrUKGbMmMHee+/dymeWyLSJT9IwSYskLZZ0aYn1vSX9RdKTkp6S9Pks4zEza68OO+wwVq5cSV1dHQ0NRNvYALV77bUXlZWVPPnkk1mFuMUyS1CSKoDrgWOB/sBISf2Liv0AmBQRhwAjgBuyisfMrD177rnn2LhxI927d+fII49k4sSJbNy4kbq6OmbNmsWQIUMaXA7QrVs37rvvPr73ve/x4IMPtu7JpLJs4hsCLI6IJQCSJgDDgYUFZQLYJZ3eFViWYTxmZplpjWHq6u9BQVI7uvXWW6moqOCEE05g9uzZHHzwwUjipz/9KR/4wAcaXP7cc88BsOeee3Lvvfdy7LHHcsstt3DooYdu+5MqoMaqfFu1Y+kkYFhEnJvOnwkcGhHnF5TZC5gB7AbsCBwdEXNL7GsUMAqgd+/eg156qayxrqwd8GPmllfPPvssBx54YGuHkXulrpOkuRHR5B93lvegSvVxUZwNRwLjI6In8Hngt5LeF1NEjIuI6oiorqpqcpRgMzNrB7JMULVAr4L5nry/Ce+rwCSAiJgNVAI9MozJzMzaiCwT1Bygn6S+kjqTPAQxuajMP4DPAEg6kCRB1WUYk5mZtRGZPSQRERsknQ9MByqAWyJigaQrgZqImAx8G7hR0jdJmv/OiaxuihVobp9ZrXET1Mw6hoV1C5suVEL/quKHo9uPTH+oGxFTgalFyy4vmF4IfCLLGMzMrG1yX3xmZpZL7urIzFpOB24/b+5PIuqt2bD5yLOTTp7U9DH7VPOvd/4FwNSpU7nwwguZOXMmU6dOZYcdduCss85i/PjxHHPMMY12XzR+/HhqamoYO3bsVp1DS3OCMjNr42bOnMkFF1zAjBkz6N27N6NHj960bvz48Rx00EHvS1DvvPPePa+1a19m/fpXN1vWkB133Hb3vJygzMzasIcffpjzzjuPqVOnst9++wEwZswYdtppJ/r06UNNTQ2nn346Xbt2Zfbs2TzzzDNceOGFvPXWKrp06cyUKbcAsHx5HV/84iheeGEpxx33GX7844sBmDnzr/zkJ9ezdu06+vbtxW9/e+emfZ999tnce++9rF+/njvuuIMDDjigRc/NCcqsQE1N85ppqqvbfhOVtT3r161n+PDhPPjggyWTw0knncTYsWO59tprqa6uZt26dZx66qlMnDiR/v135M0336Zr1y4APP30c/z1r3+gS5fOHHLIFxg9+nQqKyu55ppfc++9N7Hjjjtw3XU3cd1113H55cmzbj169OCJJ57ghhtu4Nprr+Wmm25q0fPzQxJmZm1Up+07cfjhh3PzzTeXVX7RokXstddeDB48GIBddtmJTp2SesrQoYey6647U1nZhQMO2I9//GMZc+bM57nnnufoo8/gsMNO5Lbb7qGwq7kTTzwRgEGDBvHiiy+27MnhGpSZWZsliUmTJnH00Udz1VVX8b3vfa/R8hGBVKoXOujcufOm6YqKCjZs2EhE8OlPH8b48dduWld4D6pLly4F5TdszamU5BqUmVkbtsMOOzBlyhRuu+22kjWpnXfembfeeguAAw44gGXLljFnzhwA3nrrnUYTy+DBB/PYY0/y/PNJrelf/1rN//7v/2ZwFqW5BmVm1gK2tgf95vYkAbD77rszbdo0jjzySHr02Lw703POOYfRo0dvekhi4sSJXHDBBbzzzmt07VrJvfc2fN+oqmp3fvWrn/DlL1/C2rXrAbjqqp/x4Q9/uNmxbonMhtvISnV1ddRs5W8mOvBPNdqcbT3chh+S2Eod6I+rpYfb2NZdHZXzSHkpW/qYeV6H2zAzM2s2JygzM8slJygzM8slJygzM8slJygzM8slP2a+LXSgJ5vMzFpKpglK0jDgv0hG1L0pIq4uWv9z4FPp7A7AHhHRLcuYzMwy0dwvoqm+RcNtvHB/08NtDPjAAAYMGMD69evp1KkTZ599NhdddBHbbVd+41j//p9l1qxJ9Oix2xbHnLXMEpSkCuB64LNALTBH0uR0FF0AIuKbBeUvAA7JKh6zjsiV9/atS2UX5s2bB8CKFSs47bTTeOONN7jiiitaObKWkeU9qCHA4ohYEhHrgAnA8EbKjwR+n2E8Zmbt1h577MG4ceMYO3YsEcGhhx7KggULNq0fOnQoc+fOZdWqVRxzzDEcfviXuOCCMeS5s4YsE9Q+wNKC+dp02ftI+iDQF3ggw3jMzNq1fffdl3fffZcVK1YwYsQIJk1KmgmXL1/OsmXLGDRoEFdccQVHHHEEjz56J1/4wqdYunR5K0fdsCwTVKkucxtK1SOAP0TExpI7kkZJqpFUU1dX12IBmpm1N/U1olNOOYU77rgDgEmTJnHyyScDMGvWLM444wwAhg07it1226V1Ai1DlgmqFuhVMN8TWNZA2RE00rwXEeMiojoiqquqqlowRDOzNm7hwk2vJdOnUwHssXIl+7zxBt27duWpu+9m4vjxjBg8OCm3Zg1avLi1oy5LlglqDtBPUl9JnUmS0OTiQpL2B3YDZmcYi5lZu1b36quMvvJKzj/ttE1jPo049lh+esstvPHWWwxIeyA/srqa26ZMAWDGjId57bU3Wy3mpmT2FF9EbJB0PjCd5DHzWyJigaQrgZqIqE9WI4EJkec7dWZmTdnKRx9faEZv5mvXrGXgiSeyfsMGOlVUcObxx/Ots8/etP6kY47hwquv5oejR29a9qOvfY2Rl1zCHz5xEkccUU2vXnttVdxZyvR3UBExFZhatOzyovkxWcZgZtZePf3K0/Rv5Lb8nj16sOGppzZb1r1bN2bceCPvfDCZv+aaSzOMcOu4qyMzM8sld3Vk7VNzf6H6q5YNw8yazzUoMzPLJdegzKzV1dQ0r8ZbXe0+mdoz16DMzCyXXIPaAtXjmvctz9/xzMy2nBOUmVkLaG4zZb01RcNt7L5fmcNt9Ou36XdQZw8fzkVnneXhNix7bpe31uLWgrahS2UX5t11FwArVq3itO98hzfefpsrzj+/lSNrGb4HZWbWDuzRvTvjxoxh7O23J8NtjBjBgoI+94aecw5zFyxg1euvc8x553X44TbMzGwb2rdXL96NYMWqVYw49lgmTZsGwPK6OpatWMGgj3yEK264gSMOOaTDD7dhZmbb2KbhNoYN447p0wGYNG0aJ3/ucwDMqqnhjOOOAzr2cBtmZrYNLVm6lIrttmOP7t3ZZ8896d6tG08tWsTEadMYceyxm8rV93aed05QZmbtgIfbMDOzkuqfnl245aNmALBDlYfbKOYEZWbWRnm4DTMzs1bgGpRtE80d/YJRLRqGmbUhmdagJA2TtEjSYkkl65GSTpG0UNICSbdnGY+ZWUvK849c82Brr09mNShJFcD1wGeBWmCOpMkRsbCgTD/gMuATEfGapD2yisfMrCVVVlayatUqunfv3mYe296WIoJVq1ZRWVnZ7H1k2cQ3BFgcEUsAJE0AhgOFj6qcB1wfEa8BRMSKDOMxM2sxPXv2pLa2lrq6zZ9SeOWVZu7w7eZtqLebd7i165q3XZcu5SfjyspKevbs2bwDkW2C2gdYWjBfCxxaVObDAJL+ClQAYyJiWvGOJI0ivRvRu3fvTII1M9sS22+/PX379n3f8jPPbOYORzVvw5pxzTtcza+at93AgduuS+As70GVSrPFDZKdgH7AUGAkcJOkbu/bKGJcRFRHRHVVVVWLB2pmZvmTZYKqBXoVzPcElpUoc09ErI+IF4BFJAnLzMw6uCwT1Bygn6S+kjoDI4DJRWX+CHwKQFIPkia/JRnGZGZmbURmCSoiNgDnA9OBZ4FJEbFA0pWSjk+LTQdWSVoI/AW4JCJWZRWTmZm1HZn+UDcipgJTi5ZdXjAdwLfSl5mZ2Sbu6sjMzHLJCcrMzHLJCcrMzHLJCcrMzHLJCcrMzHLJCcrMzHLJCcrMzHLJCcrMzHLJCcrMzHLJCcrMzHLJCcrMzHLJCcrMzHLJCcrMzHLJCcrMzHLJCcrMzHKprAQl6U5JX5DkhGZmZttEuQnnl8BpwN8lXS3pgHI2kjRM0iJJiyVdWmL9OZLqJM1LX+duQexmZtaOlTWibkT8GfizpF2BkcD9kpYCNwK/i4j1xdtIqgCuBz4L1AJzJE2OiIVFRSdGxPlbcxJmZtb+lN1kJ6k7cA5wLvAk8F/Ax4D7G9hkCLA4IpZExDpgAjB8q6I1M7MOo9x7UHcBDwM7AMdFxPERMTEiLgB2amCzfYClBfO16bJiX5L0lKQ/SOrVwPFHSaqRVFNXV1dOyGZm1saVW4MaGxH9I+I/ImJ54YqIqG5gG5VYFkXz9wJ9IuKjwJ+BW0vtKCLGRUR1RFRXVVWVGbKZmbVl5SaoAyV1q5+RtJukrzWxTS1QWCPqCSwrLBARqyJibTp7IzCozHjMzKydKzdBnRcRr9fPRMRrwHlNbDMH6Cepr6TOwAhgcmEBSXsVzB4PPFtmPGZm1s6V9RQfsJ0kRUTApif0Oje2QURskHQ+MB2oAG6JiAWSrgRqImIy8A1JxwMbgFdJHsIwMzMrO0FNByZJ+hXJfaTRwLSmNoqIqcDUomWXF0xfBlxWdrRmZtZhlJugvgv8H+DfSR5+mAHclFVQZmZm5f5Q912S3iR+mW04ZmZmibISlKR+wH8A/YHK+uURsW9GcZmZWQdX7lN8vyGpPW0APgX8D/DbrIIyMzMrN0F1jYiZgCLipYgYA3w6u7DMzKyjK/chiTXpUBt/Tx8dfxnYI7uwzMysoyu3BnURST983yDp7eEM4OysgjIzM2uyBpX+KPeUiLgEeBv4cuZRmZlZh9dkDSoiNgKDJJXq/NXMzCwT5d6DehK4R9IdwDv1CyPirkyiMjOzDq/cBLU7sIrNn9wLwAnKzMwyUW5PEr7vZGZm21S5PUn8hvcPNkhEfKXFIzIzM6P8Jr4pBdOVwAkUDT5oZmbWkspt4ruzcF7S70mGaDczM8tEuT/ULdYP6N2SgZiZmRUqK0FJekvSm/Uv4F6SMaKa2m6YpEWSFku6tJFyJ0kKSdXlh25mZu1ZuU18O2/pjtMeKK4HPgvUAnMkTY6IhUXldibpQunxLT2GmZm1X+XWoE6QtGvBfDdJX2xisyHA4ohYEhHrgAnA8BLl/i/wU2BNmTGbmVkHUO49qB9FxBv1MxHxOvCjJrbZB1haMF+bLttE0iFAr4gofErwfSSNklQjqaaurq7MkM3MrC0rN0GVKtdU82Cpvvs2/ZYqHb7j58C3mzp4RIyLiOqIqK6qqmqquJmZtQPlJqgaSddJ2k/SvpJ+DsxtYptaoFfBfE82/+3UzsBBwIOSXgQ+Dkz2gxJmZgblJ6gLgHXARGASsBr4ehPbzAH6SeorqTMwAphcvzIi3oiIHhHRJyL6AI8Bx0dEzRaeg5mZtUPlPsX3DtDgY+INbLMhHX13OlAB3BIRCyRdCdRExOTG92BmZh1ZuX3x3Q+cnD4cgaTdgAkR8bnGtouIqcDUomWXN1B2aDmxmJlZx1BuE1+P+uQEEBGvAXtkE5KZmVn5CepdSZu6NpLUhxK9m5uZmbWUcnsz/z7wiKSH0vkjgVHZhGRmZlb+QxLT0se/RwHzgHtInuQzMzPLRLkPSZwLXEjyW6Z5JL9Zms3mQ8CbmZm1mHLvQV0IDAZeiohPAYcA7nPIzMwyU26CWhMRawAkdYmI54D9swvLzMw6unIfkqiV1A34I3C/pNfwkO9mZpahch+SOCGdHCPpL8CuwLTMojIzsw6v3BrUJhHxUNOlzMzMtk6596DMzMy2KScoMzPLJScoMzPLJScoMzPLJScoMzPLJScoMzPLpUwTlKRhkhZJWizpfSPyShot6WlJ8yQ9Iql/lvGYmVnbkVmCklQBXA8cC/QHRpZIQLdHxICIGAj8FLguq3jMzKxtybIGNQRYHBFLImIdMAEYXlggIt4smN0RD4JoZmapLe5JYgvsAywtmK8FDi0uJOnrwLeAzjQwfIekUaQDJPbu3btUETMza2eyrEGpxLL31ZAi4vqI2A/4LvCDUjuKiHERUR0R1VVVVS0cppmZ5VGWCaoW6FUw35PGe0CfAHwxw3jMzKwNyTJBzQH6SeorqTMwAphcWEBSv4LZLwB/zzAeMzNrQzK7BxURGySdD0wHKoBbImKBpCuBmoiYDJwv6WhgPfAacHZW8ZiZWduS5UMSRMRUYGrRsssLpi/M8vhmZtZ2uScJMzPLJScoMzPLJScoMzPLJScoMzPLJScoMzPLJScoMzPLJScoMzPLJScoMzPLJScoMzPLJScoMzPLJScoMzPLJScoMzPLJScoMzPLJScoMzPLJScoMzPLJScoMzPLpUwTlKRhkhZJWizp0hLrvyVpoaSnJM2U9MEs4zEzs7YjswQlqQK4HjgW6A+MlNS/qNiTQHVEfBT4A/DTrOIxM7O2Jcsa1BBgcUQsiYh1wARgeGGBiPhLRPwrnX0M6JlhPGZm1oZkmaD2AZYWzNemyxryVeBPpVZIGiWpRlJNXV1dC4ZoZmZ5lWWCUollUbKgdAZQDfys1PqIGBcR1RFRXVVV1YIhmplZXnXKcN+1QK+C+Z7AsuJCko4Gvg8cFRFrM4zHzMzakCxrUHOAfpL6SuoMjAAmFxaQdAjwa+D4iFiRYSxmZtbGZJagImIDcD4wHXgWmBQRCyRdKen4tNjPgJ2AOyTNkzS5gd2ZmVkHk2UTHxExFZhatOzygumjszy+mZm1Xe5JwszMcskJyszMcskJyszMcskJyszMcskJyszMcskJyszMcskJyszMcskJyszMcskJyszMcskJyszMcskJyszMcskJyszMcskJyszMcskJyszMcskJyszMcskJyszMcinTBCVpmKRFkhZLurTE+iMlPSFpg6STsozFzMzalswSlKQK4HrgWKA/MFJS/6Ji/wDOAW7PKg4zM2ubshzyfQiwOCKWAEiaAAwHFtYXiIgX03XvZhiHmZm1QVk28e0DLC2Yr02XmZmZNSnLBKUSy6JZO5JGSaqRVFNXV7eVYZmZWVuQZYKqBXoVzPcEljVnRxExLiKqI6K6qqqqRYIzM7N8yzJBzQH6SeorqTMwApic4fHMzKwdySxBRcQG4HxgOvAsMCkiFki6UtLxAJIGS6oFTgZ+LWlBVvGYmVnbkuVTfETEVGBq0bLLC6bnkDT9mZmZbcY9SZiZWS45QZmZWS45QZmZWS45QZmZWS45QZmZWS45QZmZWS45QZmZWS45QZmZWS45QZmZWS45QZmZWS45QZmZWS45QZmZWS45QZmZWS45QZmZWS45QZmZWS45QZmZWS45QZmZWS5lmqAkDZO0SNJiSZeWWN9F0sR0/eOS+mQZj5mZtR2ZJShJFcD1wLFAf2CkpP5Fxb4KvBYRHwJ+DlyTVTxmZta2ZFmDGgIsjoglEbEOmAAMLyozHLg1nf4D8BlJyjAmMzNrIxQR2exYOgkYFhHnpvNnAodGxPkFZZ5Jy9Sm88+nZVYW7WsUMCqd3R9YlEnQW6cHsLLJUtaSfM1bh6/7ttfervkHI6KqqUKdMgygVE2oOBuWU4aIGAeMa4mgsiKpJiKqWzuOjsTXvHX4um97HfWaZ9nEVwv0KpjvCSxrqIykTsCuwKsZxmRmZm1ElglqDtBPUl9JnYERwOSiMpOBs9Ppk4AHIqs2RzMza1Mya+KLiA2SzgemAxXALRGxQNKVQE1ETAZuBn4raTFJzWlEVvFsA7lugmynfM1bh6/7ttchr3lmD0mYmZltDfckYWZmueQEZWZmueQE1QhJGyXNkzRf0hOSDm/mfoZKmtLS8bVlkvqkv4MrXFYt6b/T6aFNXe9S+7CWJWmMpIvLLOv3OSDpRUk9mrFdk+/5Zuzz0Zbc37aW5e+g2oPVETEQQNLngP8AjmrdkNqviKgBatLZocDbQJv+AzPbAkNp4D0vqVNEbNjSHUZEiya8bc01qPLtArwGoMTPJD0j6WlJpza2vJCkwZKelLTvNo4/tyTtm16TSyRNSTsNHg18M63BflLSnpLuTmuz8wu+aVZIulHSAkkzJHVN97mfpGmS5kp6WNIB6fLxkv5b0qOSlqQ9nlgBSd9PO3n+M0nPLV0l/a1gfR9JT6XTwyQ9J+kR4MRWCrnVSDpD0t/S9+mv0z5Im1yfXrcn0vfyzAbe8+MlXSfpL8A1knaX9EdJT0l6TNJH032NkXSLpAfT9/Q3Co7/dsH0d9LPpfmSrk6XfUPSwnSfEzK/YFsqIvxq4AVsBOYBzwFvAIPS5V8C7id5fH5P4B/AXo0sHwpMAQ4H5gK9W/vcWvsF9AGeIfkAfBIYWH+d0vVjgIsLyk8ELkqnK0h+1N0H2AAMTJdPAs5Ip2cC/dLpQ0l+YwcwHriD5MvloreQAAAIIUlEQVRZf5L+Ilv9euTlBQwCngZ2IPlSthi4OP072Dct813gB0AlsBToR9IrzKT6/7+O8AIOBO4Ftk/nbwDOAl4k6ZqoofVV6XXrmy7fPf23+D0/Pv3cqEjnfwH8KJ3+NDCvYLtHgS7pcVcVHPPt9N9j0zI7FB1zGdAlne7W2te0+OUmvsYVNvEdBvyPpIOAI4DfR8RG4J+SHgIGN7L8TZI36zjgmIgo7lGjo6oC7gG+FMlv5IY2UvbTJH/cpNf3DUm7AS9ExLy0zFygj6SdSL4M3KH3+h7uUrCvP0bEu8BCSXu22Nm0D58E7o6IfwFIqv9x/STgFOBq4NT0dQDJ9f97WvZ3vNdnZkfwGZKEPid9n3UFVpSx/uPArIh4ASAiGus95470/Q7J58uX0m0ekNRd0q7puvsiYi2wVtIKki/ItQX7ORr4Tf3/a8ExnwJuk/RH4I9beP6ZcxNfmSJiNsm3kypK9yFII8sBlgNrgENaOLS27A2Sb5Kf2Ip9rC2Y3khyX3U74PWIGFjwOrCBbdx7/vuV+nHkROAUSR8Goj4pNVC2oxBwa8F7bP+IGFPGelH+dXunaH/F6vdT6u+gONZSx/wCybBIg4C5Srqcyw0nqDKl9zAqSKrPs4BTJVVIqgKOBP7WyHKA10neDFc1UVPoSNYBXwTOknRa0bq3gJ0L5mcC/w7JWGOSdmlopxHxJvCCpJPT8pJ0cItG3n7NAk6Q1FXSzsBxABHxPMkH3w9JkhUkTd99Je2Xzo/c1sG2spnASZL2AEjvEX2wjPWzgaMk9a1fnpYvfs8XmwWcnm4zFFiZvtfLMQP4iqQdCmLZDugVEX8BvgN0A3Yqc3/bhBNU47qmNyznkfxRnp1Wt+8mqRrPBx4AvhMRrzSyHICI+CfJH/z1kg7dtqeSTxHxDvBvwDdJ7ivVu5fkg3KepE8CFwKfkvQ0SVPeR5rY9enAVyXNBxbw/rHIrISIeILkvT4PuBN4uGD1ROAMkuY+ImINSZPefelDEi9t22hbV0QsJLkXNyN9aOR+knvOja6PiDqS63ZX+v6sT/jF7/liY4DqdF9X814/puXEOo2k79Oa9PPsYpIv3L9L/6aeBH4eEa+XfQG2AXd1ZGZmueQalJmZ5ZITlJmZ5ZITlJmZ5ZITlJmZ5ZITlJmZ5ZITlOVW+kv5eenrFUkvF8x3LnMfv5G0fxNlvi7p9K2M9Ycq0fdi3kj6saSLMj7G9PQ3VGZbxY+ZW5sgaQxJv2LXFi0Xyfv43VYJ7L04ZgFfbKLbmubuu6Kgu5ut3dePSX7g+Z8tsT+zLLkGZW2OpA8p6TH+V8ATwF6SxkmqUdKr+eUFZR+RNFBSJ0mvS7o67c15dsEv/DfVKtLyVyvpgXqR0l7TJe0o6c5029+nx6rvp7EbJP2bSRqRxjZfSS/UpL0y3KqkJ+knJB2ZLj9X0n8WxDpN0hEFsf5YSS/iQyQdmsY8X9LjknZIy12XxvqUpHMbuF6Xp+dyP0nHrvXLP5bu66n03HYtuAbXKekFfqGScbrulvT39ItC/fb3KuktfkHhsSXVSupW8P90c1rmT5Iq0zLfTPc9X0kffmbv4wRlbVV/4OaIOCQiXgYujYhq4GDgs5L6l9hmV+ChiDiYpLuZrzSwb0XEEOASoD7ZXQC8km57NZv3qXgM8Od0+kfAZ9JyJ6TLvgGsi4gBwJnAb8tootwVeCKN40lgAvD1dL/HkPS9NgpYkZYZDHxdUu/NTkQaQtLB6EDgJGBIwerfAd+OiI8Ci0i6Maq3OiI+CdxM0onoaGAAMKo+IZP0rDIoPfa3lHTeW2x/4D8j4iPAapKurSDpWmdgej7nN3EtrINygrK26vmImFMwP1LSEyQ1qgNJElix1RHxp3R6LslwHaXcVaLMESRJgoio7z6p3jCgfr9/Jen1/lze+/s6Avhtuu0CkiEOPtT46bGOpOss0vP5R9oNERHxRtrkdwzw5bTrmsdJ+lLrV7SfI4E7I2J1RLxB0p0OkroDlRHxSFru1rRsvfpezJ8Gno6If6ZdG70I9EzXfVNJVz2z02X78X6LI+LpdLrwei4g6WbndGB9E9fCOqhc9VxrtgU29fIsqR9JX31DIuL1tMmossQ26wqmS/X4XG9tiTKN9Xo+iOTDF+A8kvGn/g2Yr2RQuYa23cDmXxILY14d790gbqgnagFfi4iZjcRGI9s2pv4avMvmPWW/C3SSdDRJQvt4RKxW0hdfqWveUC/bnyMZnXo48ANJB7XUfTZrP1yDsvZgF5KeoN+UtBfJh19Le4RkPCQkDSCtoSnpJf3pgoc09o2Ix0iay14D9mHzXqgPJOlQdDFJbeQQJfqQJLpSFgAflPSxdB+7KBmZdTrwNaVDJEjaX+mIwgVmASdKqlTSA/y/AUTESmC13huZ+EzgoS24HrsCr6bJ6SMkzXxlSWPvGREPkDSjVpEMkGi2GdegrD14AlhIMkLvEpJmtpb2C5Kmu6fS4z1DMp7VqcC0gnI/VzKMgoAZEfGMpOeBXyvpNXo9cFZErFMyoOXLJM1oz5D0IP4+EbFW0kjgl+lDBqtJBnD8NdAbmKdkQLwVFPXaHhF/k3Q3SQ/7L5IkrHpnpvvsSpIwv7wF1+M+kvtR80mG3Xh8C7btBNyu5FH07YBrIuKtLdjeOgg/Zm5WhrSW0iki1qRNijNI7vfMAE5Nh1AwsxbkGpRZeXYCZqaJSsD/iYgNJDUZM8uAa1BmZpZLfkjCzMxyyQnKzMxyyQnKzMxyyQnKzMxyyQnKzMxy6f8D5S54t6b7nVEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#creating a bar chart\n",
    "\n",
    "#data to plot\n",
    "n_groups = 4\n",
    "means_book = [0.7850, 0.6150, 0.7100, 0.6275]\n",
    "means_kitchen = [0.6600, 0.8300, 0.6525, 0.7850]\n",
    "means_dvd = [0.7175, 0.6750, 0.7750, 0.6925]\n",
    "means_electronics = [0.6525, 0.7625, 0.6650, 0.7700]\n",
    "\n",
    "#create plot \n",
    "fig, ax = plt.subplots()\n",
    "index = np.arange(n_groups)\n",
    "bar_width = 0.15\n",
    "opacity= 0.8\n",
    "\n",
    "rects1 = plt.bar(index, means_book, bar_width,\n",
    "                alpha=opacity,\n",
    "                color='b',\n",
    "                label='Book')\n",
    "\n",
    "rects2 = plt.bar(index + bar_width, means_kitchen, bar_width,\n",
    "                alpha=opacity,\n",
    "                color='g',\n",
    "                label='Kitchen')\n",
    "\n",
    "rects3 = plt.bar(index + bar_width*2, means_dvd, bar_width,\n",
    "                alpha=opacity,\n",
    "                color='r',\n",
    "                label='Dvd')\n",
    "\n",
    "rects3 = plt.bar(index + bar_width*3, means_electronics, bar_width,\n",
    "                alpha=opacity,\n",
    "                color='y',\n",
    "                label='Dvd')\n",
    "\n",
    "plt.xlabel('Training/source domains')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('possible combinations')\n",
    "plt.xticks(index + bar_width, ('Book', 'kitchen', 'dvd', 'electronics'))\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the table and graph above show the 16 possible combinations of training and testing from the 4 different domains.\n",
    "the diagrams consistently suggest that when each domain was trained and tested on itself, it had a higher accuracy.\n",
    "\n",
    "so for instance when book was trained and tested on book, it had the highest accuracy than when it was trained on book and tested on kitchen, dvd, or electronics.\n",
    "this shows that the training data is more accurate when tested on its own source domain, as oppose to being tested on another domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Adapt your code so that you can build and use a training set built from multiple categories.  Now **investigate** how having a mixture of source domains affects the accuracy of the Naive Bayes classifier on the target domain.  Make sure you control for or consider how much any improvements are due to the quantity of the training data. \\[9 marks\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-186-572d5e82c5ef>, line 33)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-186-572d5e82c5ef>\"\u001b[0;36m, line \u001b[0;32m33\u001b[0m\n\u001b[0;31m    as shown in the data above, when the dvd training was tested on its own dvd testing, the accuracy\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#mutliple training on 1 testing \n",
    "\n",
    "#1 training set on its OWN testing set    \n",
    "def combinations01(category1, r):\n",
    "    training1, testing1 = get_training_test_data(category1, r)\n",
    "    classifier = NaiveBayesClassifier.train(training1)\n",
    "    return accuracy(classifier, testing1)\n",
    "\n",
    "\n",
    "#2 training sets on 1 other testing document \n",
    "def combinations03(category1, category2, category3, r):\n",
    "    training1, testing1 = get_training_test_data(category1, r)\n",
    "    training2, testing2 = get_training_test_data(category2, r)\n",
    "    training3, testing3 = get_training_test_data(category3, r)\n",
    "    classifier = NaiveBayesClassifier.train(training1 + training2)\n",
    "    return accuracy(classifier, testing3)\n",
    "\n",
    "#3 training sets on 1 other testing document\n",
    "def combinations04(category1, category2, category3, category4, r):\n",
    "    training1, testing1  = get_training_test_data(category1, r)\n",
    "    training2, testing2 = get_training_test_data(category2, r)\n",
    "    training3, testing3 = get_training_test_data(category3, r)\n",
    "    training4, testing4 = get_training_test_data(category4, r)\n",
    "    classifier = NaiveBayesClassifier.train(training1 + training2 + training3)\n",
    "    return accuracy(classifier, testing4)\n",
    "    \n",
    "\n",
    "print(combinations01(\"dvd\", 0.8)) \n",
    "print(combinations03(\"book\", \"kitchen\", \"dvd\", 0.8/2))\n",
    "print(combinations04(\"book\", \"kitchen\", \"electronics\", \"dvd\", 0.8/3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "above is an example of singular, and multiple training sets tested on the same 'dvd' source domain to see whether the \n",
    "results vary when multiple training sets test on one constant test set, which in this case is dvd.\n",
    "\n",
    "when the dvd was trained using its own dvd documents, the accuracy was higher than that of conjoined training sets.\n",
    "the pattern shown above, shows that the more training sets are used in combination to train the classifier on a constant testing set,\n",
    "the less accurate the classifier is in its predictions of classifiying features within that document.\n",
    "\n",
    "for instance, when the three training sets of book, kithchen, and electronics were tested on dvd, the accuracy showed to \n",
    "be lower than that of when two training sets(book and kitchen) were used.\n",
    "\n",
    "the highest accuracy given was when dvd was trained and tested on itself. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Document Similarity (25 marks)\n",
    "The objective of this question is to investigate whether incorporating lexical knowledge from WordNet might improve document similarity methods.  For example, knowing that both *tiger* and *leopard* are hyponyms of *big_cat* should increase the similarity between a document mentioning a *tiger* and a document mentioning a *leopard*.\n",
    "\n",
    "The code below will generate two document collections, both in bag-of-words format, one from the Medline Corpus and one from the Wall Street Journal corpus.\n",
    "\n",
    "In this question, there are marks available for the quality of your code and the quality of your explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sussex_nltk.corpus_readers import MedlineCorpusReader\n",
    "from sussex_nltk.corpus_readers import WSJCorpusReader\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "def normalise(tokenlist):\n",
    "    tokenlist=[token.lower() for token in tokenlist]\n",
    "    tokenlist=[\"NUM\" if token.isdigit() else token for token in tokenlist]\n",
    "    tokenlist=[\"Nth\" if (token.endswith((\"nd\",\"st\",\"th\")) and token[:-2].isdigit()) else token for token in tokenlist]\n",
    "    tokenlist=[\"NUM\" if re.search(\"^[+-]?[0-9]+\\.[0-9]\",token) else token for token in tokenlist]\n",
    "    return tokenlist\n",
    "\n",
    "def filter_stopwords(tokenlist):\n",
    "    stop = stopwords.words('english')\n",
    "    return [w for w in tokenlist if w.isalpha() and w not in stop]\n",
    "\n",
    "def stem(tokenlist):\n",
    "    st=WordNetLemmatizer()\n",
    "    return [st.lemmatize(token) for token in tokenlist]\n",
    "\n",
    "   \n",
    "def make_bow(somestring):\n",
    "    rep=word_tokenize(somestring)  #step 1\n",
    "    rep=normalise(rep)   #step 2\n",
    "    rep=stem(rep)   #step 3\n",
    "    rep=filter_stopwords(rep)  #step 4\n",
    "    dict_rep={}\n",
    "    for token in rep:\n",
    "        dict_rep[token]=dict_rep.get(token,0)+1  #step 5\n",
    "    return(dict_rep)\n",
    "\n",
    "wsj=WSJCorpusReader()\n",
    "medline=MedlineCorpusReader()\n",
    "\n",
    "collectionsize=50\n",
    "collections={\"wsj\":[],\"medline\":[]}\n",
    "\n",
    "for key in collections.keys():\n",
    "    if key==\"wsj\":\n",
    "        generator=wsj.raw()\n",
    "    else:\n",
    "        generator=medline.raw()\n",
    "    while len(collections[key])<collectionsize:\n",
    "        collections[key].append(next(generator))\n",
    "\n",
    "bow_collections={key:[make_bow(doc) for doc in collection] for key,collection in collections.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_bow(\"the 2nd GROUP of girls who came in at 4 and 0.32 seconds!!?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a). For each step in the `make_bow()` function, **explain** what it does and why it is applicable when creating document representations for document similarity methods. \\[8 marks\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The make_bow method takes in a parameter of any string. \n",
    "#step 1\n",
    "A varible 'rep' is initialized in the first step of the method. the string (the argument the method takes in)\n",
    "will be tokenized and stored in the varible 'rep'\n",
    "\n",
    "#step 2\n",
    "the value of the variable 'rep' is updated to call the method 'normalise', which normalises the tokenList in the string. \n",
    "when normalise is called, it will take the value of 'rep' as its argument, in the make_bow() function,the value that will be returned from this method will\n",
    "    be stored in 'rep'\n",
    "\n",
    "    #normalise\n",
    "    the method normalise, takes in a parameter of tokenList,\n",
    "    #step 1\n",
    "    the first step of the method, creates a for loop, which iterates for every token in that tokenList(that was the parameter of the method)\n",
    "    each token in that tokenlist will be converted into lowercase letters and is stored into the tokenlist, which is \n",
    "returned into the variable 'rep'.\n",
    "    this is applicable when creating document simalarity since it is easier to detect simarlity when all tokenized words are stored in the same form,\n",
    "    given that form does not change the meaning of the token.\n",
    "    there is no difference in sentiment between an upper and lowercase word, example: \"GROUP\" and \"group\", \n",
    "    it would not make a string more or less similar than another string containing the same word in lowercase, so all are kept in the same format.   \n",
    "        \n",
    "    #step2\n",
    "    the second step of the normalise function, checks for whole number integers in the inputted string, using the isDigit() method in an if and else statement.\n",
    "    if the string does contain an integer, this integer becomes of type NUM, and is stored into the tokenlist, which is returned to the variable 'rep'.\n",
    "    \n",
    "    #step3\n",
    "    the third step of the normalise function uses an if and else statement to check if a token in the tokenlist\n",
    "    ends with the following characters: 'nd', 'st', 'th', and if the if the token which ends in either of those characters is \n",
    "        a digit. if both of these statements are true, that token is categorized of type 'Nth'. \n",
    "        an example of this would be, a string,(which is converted to a tokenlist) similar to the following:\n",
    "        \"the 2nd GROUP of girls who came in at 4\"\n",
    "        2nd will become of type Nth.\n",
    "        \n",
    "    #step4 \n",
    "    the final step of the normalise method checks for numbers that are not whole numbers, for instance decimals, negative numbers. \n",
    "    if a decimal/negative number etc. is found, it becomes of type NUM as well.\n",
    "    this has been written on its own line, since the isdigit() method used in step2 only checks for integers(whole numbers)\n",
    "    this is relevant in document simiarity, because if given multiple documents, an existence of numbers in general gives two documents \n",
    "    a similarity, however, whether the number is signed, decimal or a fraction is trivial, and does not make a difference in indentifying simalrity,\n",
    "    therefore all types of numbers are grouped under the same type, NUM.\n",
    "\n",
    "\n",
    "#step3\n",
    "the origin, or singular form of a token in the tokenlist is stored in the variable rep.\n",
    "for instance, if the tokenized string was \"girls\", it would be stored as \"girl\". this makes it easier to indentify simalrity,\n",
    "because it is more likely to detect two words if they are both stored in the same singular form\n",
    "as oppose to having one in its plural form and not the other.\n",
    "\n",
    "#step4\n",
    "stop words, like (if, the, and, so, a) are eliminated from the tokenlist. this is because they do not contribute to a specific sentiment or meaning.\n",
    "two documents can share a lot of the same connectives, or stop words and have nothig in common.\n",
    "for instance:\n",
    "    string_1: \"the cat and the boy took a walk\"\n",
    "    string_2: \"the science article shows how a cigerattes a day can lead to cancer\"\n",
    "both strings share the stopwords 'the','and','a','to', however they do not share the same meaning or sentiment.\n",
    "\n",
    "#STEP5\n",
    "an empty set 'dict_rep' is initilaised, and for every token stored in the variable rep,\n",
    "the pointer increment by one, on the position it is pointing towards, adding all the filtered tokens\n",
    "into the dict_rep set.\n",
    "\n",
    "If the method make_bow() is called on the following string:\n",
    "\"the 2nd GROUP of girls who came in at 4 and 0.32 seconds!!?\"\n",
    "\n",
    "each word in that string will be tokenized, and stored into the variable 'rep',\n",
    "the method turn all characters to lowercase, so \"GROUP\", will become \"group\".\n",
    "2nd, will become of type Nth\n",
    "4 and 0.32 will both become of type NUM\n",
    "'girls' will be stored as girl \n",
    "'the', 'of', 'in', 'at', 'and' will be filtered out since they are stopwords \n",
    "and the remaining tokens will be added to the set dict_rep{}\n",
    "\n",
    "the following is the output after running the string above:\n",
    "\n",
    "{'Nth': 1, 'group': 1, 'girl': 1, 'came': 1, 'NUM': 2, 'second': 1}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b). Apply a TF-IDF weighting to the representations and then compute: \n",
    "* the average cosine similarity of medline documents to each other, \n",
    "* the average cosine similarity of WSJ documents to each other,\n",
    "* the average cosine similarity of medline documents to WSJ documents\n",
    "\\[8 marks\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math   \n",
    "def dot(docA,docB):\n",
    "    the_sum= 0\n",
    "    for (key,value) in docA.items():\n",
    "        the_sum+=value*docB.get(key,0)\n",
    "    return the_sum\n",
    "        \n",
    "        \n",
    "def cos_sim(docA,docB):\n",
    "    sim=dot(docA,docB)/(math.sqrt(dot(docA,docA)*dot(docB,docB)))\n",
    "    return sim\n",
    "\n",
    "def doc_freq(doclist):\n",
    "    df={}\n",
    "    for sentence in doclist:\n",
    "        for feat in sentence.keys():\n",
    "            df[feat]=df.get(feat,0)+1\n",
    "    return df\n",
    "\n",
    "\n",
    "            \n",
    "#term-frequency tf - the number of occurences of term t in document d\n",
    "#docuement frequency df- number of documents in collection containing term t\n",
    "#inverse document frequency -> idf = log(N/df) - where \n",
    "#where N is the total no. of documents\n",
    "#term frequency-inverse document frequency - tf - idf = tf * idf\n",
    "\n",
    "def idf(doclist):\n",
    "    N=len(doclist)\n",
    "    return {feat:math.log(N/v) for feat, v in doc_freq(doclist).items()}\n",
    "\n",
    "\n",
    "def tf_idf(doclist,idf):\n",
    "    documentFreq = doc_freq(doclist) \n",
    "    tfidf=[]\n",
    "    for doc in doclist:\n",
    "        new_doc={}\n",
    "        for feat in doc:\n",
    "            new_doc[feat]=idf[feat]*documentFreq[feat]\n",
    "        tfidf.append(new_doc)\n",
    "    return tfidf\n",
    "\n",
    "\n",
    "def avg_sim(collection_1,collection_2):\n",
    "    count_cos_sim = 0\n",
    "    for doc_1 in collection_1:\n",
    "        for doc_2 in collection_2:\n",
    "            count_cos_sim += cos_sim(doc_1,doc_2)\n",
    "            #count_cos_sim = count_cos_sim + x\n",
    "    return count_cos_sim/2500\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NUM': 37, 'early': 3, 'case': 3, 'organ': 1, 'transplantation': 1, 'section': 1, 'healthy': 1, 'ovary': 1, 'fertile': 1, 'woman': 1, 'unintentionally': 1, 'unknowingly': 1, 'subjected': 1, 'oophorectomy': 1, 'described': 4, 'paper': 1, 'concentrate': 1, 'surgical': 1, 'technique': 2, 'outcome': 1, 'preliminary': 2, 'time': 11, 'publication': 1, 'surgeon': 1, 'involved': 4, 'found': 11, 'donor': 1, 'willing': 1, 'lose': 1, 'portion': 2, 'ovarian': 1, 'tissue': 2, 'various': 4, 'reason': 1, 'proven': 1, 'recipient': 1, 'wa': 22, 'made': 4, 'ready': 1, 'concurrently': 1, 'storage': 1, 'necessary': 1, 'material': 3, 'stored': 1, 'saline': 1, 'period': 2, 'upon': 6, 'reintroduction': 1, 'pelvic': 1, 'cavity': 2, 'immediately': 1, 'became': 1, 'reinfused': 1, 'appeared': 2, 'neither': 4, 'begun': 1, 'menstruating': 1, 'postsurgery': 1, 'article': 1, 'month': 2, 'postoperatively': 1, 'sign': 1, 'hopeful': 1, 'spelled': 1, 'manner': 1, 'three': 10, 'group': 13, 'male': 1, 'subject': 1, 'average': 3, 'fitness': 1, 'af': 1, 'n': 3, 'high': 5, 'hf': 1, 'highly': 4, 'fit': 1, 'competitive': 1, 'race': 1, 'walker': 1, 'crw': 1, 'performed': 3, 'maximal': 3, 'treadmill': 1, 'test': 2, 'walking': 1, 'mph': 1, 'running': 1, 'addition': 12, 'oxygen': 4, 'uptake': 6, 'max': 1, 'exception': 2, 'modified': 1, 'balke': 1, 'discontinuous': 1, 'nature': 2, 'obtained': 3, 'similar': 8, 'regardless': 1, 'speed': 2, 'within': 6, 'significantly': 1, 'lower': 4, 'range': 5, 'resulted': 4, 'level': 9, 'greater': 2, 'equal': 1, 'associated': 4, 'physiological': 2, 'variable': 1, 'heart': 1, 'rate': 9, 'ventilation': 1, 'respiratory': 1, 'exchange': 1, 'ratio': 4, 'demonstrate': 1, 'discernable': 1, 'pattern': 5, 'reference': 1, 'mode': 1, 'locomotion': 1, 'versus': 1, 'concluded': 2, 'elicited': 1, 'independent': 1, 'le': 5, 'interrelated': 1, 'state': 3, 'training': 1, 'polysaccharide': 1, 'cause': 3, 'inhibition': 3, 'multiplication': 2, 'mumps': 1, 'virus': 3, 'allantoic': 1, 'sac': 1, 'may': 5, 'hemagglutination': 1, 'moreover': 2, 'substance': 1, 'prevent': 1, 'adsorption': 2, 'erythrocyte': 2, 'available': 2, 'evidence': 4, 'indicates': 1, 'active': 6, 'inhibitor': 2, 'block': 2, 'cell': 20, 'living': 2, 'membrane': 10, 'influenza': 1, 'b': 3, 'newcastle': 1, 'disease': 1, 'well': 7, 'pvm': 1, 'also': 12, 'appears': 6, 'lack': 2, 'correlation': 2, 'vitro': 3, 'vivo': 1, 'inhibiting': 1, 'activity': 16, 'escherichia': 15, 'coli': 17, 'c': 7, 'strain': 15, 'containing': 4, 'different': 6, 'deoxyribonucleic': 5, 'acid': 17, 'dna': 5, 'synthesis': 9, 'mutation': 9, 'tested': 3, 'support': 4, 'bacteriophage': 4, 'satellite': 1, 'phage': 2, 'requires': 2, 'functional': 1, 'dnab': 1, 'dnae': 1, 'dnag': 1, 'gene': 4, 'product': 2, 'whereas': 5, 'doe': 4, 'require': 1, 'dnaa': 1, 'dnac': 1, 'dnah': 1, 'contrast': 4, 'need': 1, 'thus': 9, 'genome': 1, 'replicated': 1, 'differently': 1, 'even': 3, 'though': 3, 'packaged': 1, 'head': 1, 'protein': 7, 'regulation': 1, 'glutamate': 2, 'dehydrogenase': 5, 'ec': 4, 'glutamine': 1, 'synthetase': 1, 'synthase': 1, 'examined': 5, 'culture': 4, 'salmonella': 3, 'typhimurium': 3, 'grown': 5, 'nitrogen': 3, 'amino': 5, 'source': 3, 'regulatory': 2, 'observed': 10, 'klebsiella': 2, 'aerogenes': 3, 'decrease': 4, 'derepressed': 1, 'growth': 11, 'limiting': 1, 'ammonia': 1, 'regulate': 1, 'reported': 3, 'increase': 4, 'however': 6, 'auxotroph': 2, 'complex': 3, 'enzyme': 17, 'decreasing': 1, 'auxotrophs': 1, 'either': 7, 'procedure': 5, 'us': 2, 'indicator': 2, 'plasmid': 3, 'identify': 1, 'bacterial': 3, 'population': 1, 'second': 3, 'lacking': 1, 'detectable': 3, 'phenotypic': 1, 'property': 4, 'appropriate': 2, 'condition': 4, 'indirect': 1, 'selection': 1, 'transformants': 1, 'carrying': 3, 'contain': 4, 'nonselected': 1, 'replication': 1, 'function': 3, 'enables': 1, 'elimination': 1, 'doubly': 1, 'transformed': 1, 'bacteria': 2, 'using': 2, 'isolated': 8, 'carry': 2, 'small': 3, 'cryptic': 1, 'genetic': 4, 'element': 4, 'contains': 4, 'nucleotide': 4, 'pair': 3, 'capable': 2, 'functioning': 1, 'replicon': 1, 'independently': 1, 'two': 11, 'larger': 3, 'normally': 2, 'ikeda': 1, 'inuzuka': 1, 'tomizawa': 1, 'effect': 12, 'elevated': 1, 'temperature': 7, 'studied': 6, 'five': 4, 'enterobacteriaceae': 1, 'shift': 1, 'immediate': 1, 'due': 5, 'limitation': 2, 'availability': 1, 'endogenous': 1, 'methionine': 4, 'first': 4, 'biosynthetic': 1, 'extract': 2, 'aerobacter': 1, 'shown': 5, 'sensitive': 5, 'anacystis': 1, 'nidulans': 1, 'treatment': 4, 'precursor': 1, 'de': 2, 'novo': 2, 'pyrimidine': 1, 'pathway': 3, 'compound': 2, 'salvage': 1, 'degradative': 1, 'could': 8, 'replace': 1, 'uracil': 1, 'requirement': 2, 'reversion': 2, 'frequency': 1, 'nonuracil': 1, 'calculated': 5, 'utilization': 2, 'mol': 2, 'per': 4, 'unit': 2, 'amount': 4, 'required': 4, 'mass': 1, 'study': 6, 'characteristic': 3, 'pt': 1, 'region': 2, 'led': 2, 'recognition': 1, 'previously': 3, 'undescribed': 1, 'phenotype': 1, 'grow': 3, 'carbon': 1, 'unable': 1, 'utilize': 1, 'succinate': 4, 'effectively': 1, 'separated': 4, 'genetically': 2, 'tightly': 1, 'linked': 1, 'mutant': 11, 'yield': 3, 'interesting': 1, 'class': 4, 'suppressor': 1, 'pseudomonas': 2, 'fluorescens': 2, 'w': 1, 'maltose': 1, 'exclusively': 1, 'hydrolyzing': 1, 'glucose': 2, 'via': 2, 'inducible': 2, 'glucohydrolase': 1, 'phosphorolytic': 1, 'cleavage': 4, 'oxidation': 1, 'maltobionic': 1, 'organism': 4, 'totally': 1, 'intracellular': 3, 'ph': 8, 'induction': 2, 'occurred': 2, 'incubated': 1, 'maltotriose': 1, 'rapid': 2, 'easily': 2, 'min': 4, 'inducer': 1, 'derivative': 2, 'repress': 1, 'growing': 2, 'plus': 3, 'exhibited': 2, 'alone': 2, 'messenger': 1, 'ribonucleic': 4, 'significant': 4, 'derepression': 1, 'serine': 1, 'hydroxymethyltransferase': 1, 'mete': 1, 'metf': 1, 'sulfoxide': 1, 'instead': 1, 'prevented': 4, 'glycine': 1, 'adenosine': 4, 'guanosine': 3, 'thymidine': 1, 'medium': 8, 'showing': 3, 'secondary': 2, 'deficiency': 2, 'nutrient': 1, 'hand': 2, 'meta': 1, 'lead': 1, 'met': 1, 'regulon': 1, 'marginal': 1, 'prototrophic': 1, 'metj': 1, 'minimal': 2, 'ha': 12, 'wild': 2, 'type': 3, 'influenced': 1, 'partially': 5, 'repressed': 1, 'metk': 1, 'twice': 1, 'much': 4, 'supplemented': 2, 'result': 12, 'show': 9, 'depression': 1, 'system': 3, 'one': 9, 'control': 2, 'cytoplasmic': 1, 'separate': 2, 'mitochondrial': 2, 'rho': 1, 'saccharomyces': 3, 'cerevisiae': 3, 'killer': 2, 'character': 1, 'k': 3, 'psi': 1, 'griffith': 1, 'suggested': 3, 'interaction': 2, 'venr': 1, 'tetr': 1, 'possibly': 1, 'located': 2, 'marker': 1, 'possible': 3, 'best': 1, 'characterized': 2, 'investigated': 3, 'nonkiller': 1, 'segregants': 1, 'nks': 1, 'suitable': 1, 'mated': 1, 'x': 2, 'combination': 1, 'difference': 5, 'quantitative': 2, 'transmission': 1, 'cross': 1, 'illustrating': 1, 'phenomenon': 5, 'bias': 1, 'polarity': 1, 'suppressiveness': 1, 'intercellular': 1, 'influence': 1, 'genetics': 1, 'smaller': 2, 'detected': 3, 'ribonuclease': 4, 'iii': 4, 'rnase': 2, 'kindler': 1, 'et': 1, 'al': 1, 'mapped': 3, 'use': 3, 'f': 1, 'merodiploids': 1, 'hfr': 1, 'mating': 2, 'transduction': 2, 'lie': 1, 'close': 2, 'nadb': 2, 'near': 3, 'map': 3, 'transferred': 2, 'original': 2, 'background': 1, 'conjugation': 1, 'new': 3, 'defect': 1, 'processing': 1, 'slowly': 2, 'parental': 1, 'seems': 2, 'depend': 2, 'grows': 1, 'equally': 1, 'female': 1, 'specific': 6, 'cut': 1, 'make': 1, 'defined': 1, 'seem': 2, 'developed': 1, 'useful': 1, 'labeling': 1, 'selective': 2, 'manipulation': 1, 'used': 2, 'determine': 1, 'biotin': 1, 'histidine': 5, 'position': 1, 'expected': 2, 'known': 3, 'apparently': 1, 'possibility': 2, 'discussed': 4, 'urap': 1, 'transferring': 1, 'somewhat': 2, 'removed': 2, 'commonly': 1, 'accepted': 1, 'localization': 2, 'affecting': 2, 'attempted': 1, 'series': 2, 'experiment': 2, 'another': 4, 'based': 3, 'data': 6, 'order': 2, 'chromosome': 2, 'tyra': 1, 'rana': 1, 'rnc': 1, 'puri': 1, 'fail': 2, 'enriched': 1, 'defective': 2, 'corresponding': 4, 'reduces': 1, 'form': 7, 'plaque': 2, 'efficiency': 1, 'suggest': 5, 'beneficial': 1, 'normal': 4, 'higher': 3, 'becomes': 1, 'indispensable': 1, 'transport': 6, 'epsilon': 1, 'increased': 2, 'response': 1, 'starvation': 2, 'ammonium': 1, 'sulfate': 3, 'cycloheximide': 2, 'slow': 2, 'loss': 3, 'added': 3, 'h': 2, 'presence': 7, 'yeast': 2, 'nicotinamide': 3, 'adenine': 4, 'dinucleotide': 2, 'anabolic': 1, 'double': 2, 'depressed': 1, 'catabolic': 1, 'sensitivity': 2, 'ion': 2, 'metabolite': 1, 'subsequent': 1, 'incorporation': 3, 'dissimilates': 1, 'glycerol': 1, 'aerobically': 1, 'initiated': 1, 'kinase': 1, 'convert': 1, 'substrate': 6, 'phosphorylated': 2, 'dehydrogenated': 1, 'dihydroxyacetone': 1, 'phosphate': 2, 'flavoprotein': 2, 'anaerobically': 1, 'keto': 1, 'kind': 2, 'constitutive': 1, 'aerobic': 1, 'anaerobic': 1, 'peptide': 2, 'hydrolase': 1, 'purified': 10, 'bacillus': 4, 'subtilis': 3, 'distinguished': 1, 'respect': 2, 'molecular': 7, 'weight': 8, 'catalytic': 2, 'relation': 1, 'physiology': 1, 'bacterium': 1, 'designated': 1, 'aminopeptidase': 1, 'produced': 4, 'hydrolyzes': 1, 'rapidly': 3, 'ii': 2, 'third': 1, 'predominantly': 3, 'stationary': 1, 'phase': 2, 'efficiently': 1, 'utilizes': 1, 'suggests': 2, 'catabolism': 1, 'occurs': 5, 'perhaps': 2, 'related': 2, 'cessation': 1, 'onset': 1, 'event': 1, 'dipeptide': 1, 'identified': 3, 'localized': 2, 'wall': 4, 'periplasm': 1, 'variation': 3, 'cycle': 1, 'important': 2, 'antibiotic': 3, 'metabolism': 1, 'protoplast': 1, 'l': 1, 'eight': 1, 'lysogens': 1, 'qualitatively': 2, 'quantitatively': 1, 'removal': 2, 'still': 1, 'adsorb': 1, 'nine': 1, 'kill': 1, 'host': 1, 'multiply': 2, 'naked': 1, 'forming': 2, 'lawn': 1, 'individual': 4, 'similarly': 1, 'pleiotropic': 2, 'strongly': 3, 'dependent': 1, 'plating': 1, 'gta': 1, 'glucosylation': 1, 'teichoic': 1, 'site': 8, 'phi': 1, 'phie': 1, 'unstabilized': 1, 'lysogenized': 1, 'exhibit': 2, 'resistance': 2, 'basis': 3, 'ordered': 1, 'four': 1, 'classified': 1, 'six': 2, 'together': 1, 'partly': 1, 'metabolic': 1, 'development': 2, 'vesicle': 2, 'prepared': 2, 'antibody': 2, 'french': 1, 'press': 1, 'completely': 3, 'inhibited': 3, 'suggesting': 3, 'outside': 1, 'previous': 2, 'futai': 1, 'indicate': 3, 'inversion': 1, 'proline': 1, 'insensitive': 1, 'inside': 2, 'essentially': 4, 'confirms': 1, 'short': 1, 'kaback': 1, 'kohn': 1, 'unlike': 1, 'represent': 2, 'inverted': 1, 'preparation': 3, 'ferricyanide': 1, 'reductase': 1, 'spheroplasts': 1, 'total': 2, 'ethylenediaminetetraacetate': 1, 'finding': 2, 'slightly': 2, 'half': 1, 'reactive': 1, 'moved': 2, 'remains': 2, 'supernatant': 3, 'torulopsis': 1, 'glabrata': 1, 'contained': 5, 'glycoprotein': 1, 'toxin': 1, 'killed': 1, 'kinetics': 2, 'treated': 1, 'showed': 1, 'leakage': 1, 'cellular': 1, 'potassium': 2, 'partial': 3, 'dissipation': 1, 'triphosphate': 2, 'pool': 2, 'coordinate': 1, 'shutdown': 1, 'macromolecular': 1, 'least': 1, 'toxic': 1, 'stable': 1, 'killing': 1, 'eighteen': 1, 'covalently': 2, 'closed': 2, 'circular': 2, 'duplex': 1, 'homogeneous': 3, 'size': 4, 'buoyant': 1, 'density': 3, 'atcc': 1, 'copy': 1, 'closely': 1, 'biochemical': 1, 'criterion': 1, 'pumilus': 1, 'respectively': 3, 'present': 4, 'several': 7, 'clinical': 1, 'isolates': 1, 'bacteroides': 1, 'sedimentation': 3, 'alkaline': 1, 'sucrose': 2, 'gradient': 1, 'cscl': 1, 'ethidium': 1, 'bromide': 1, 'equilibrium': 7, 'centrifugation': 2, 'electron': 5, 'microscopy': 2, 'bacteriodes': 1, 'fragilis': 1, 'specie': 4, 'ochraceus': 1, 'distinct': 1, 'cosedimented': 1, 'r': 2, 'sedimented': 1, 'molecule': 6, 'role': 1, 'unknown': 2, 'bacteriocins': 1, 'deficient': 3, 'uridyl': 1, 'transferase': 1, 'converting': 1, 'galactose': 1, 'co': 1, 'leloir': 1, 'seven': 2, 'inactivated': 2, 'interface': 1, 'inactivation': 1, 'whether': 3, 'brought': 1, 'shaking': 1, 'bubbling': 1, 'reaction': 4, 'enough': 1, 'maintain': 1, 'saturated': 1, 'analogy': 1, 'surface': 2, 'denaturation': 1, 'pointed': 1, 'apparent': 3, 'permeability': 1, 'mutagenesis': 1, 'number': 1, 'detergent': 2, 'unrelated': 1, 'presumably': 1, 'affect': 2, 'biosynthesis': 1, 'acra': 1, 'mtc': 1, 'locus': 1, 'approximately': 2, 'taylor': 1, 'trotter': 1, 'barotolerant': 1, 'ribosomal': 1, 'subunit': 3, 'conditionally': 1, 'lethal': 1, 'structural': 2, 'ligase': 1, 'extensive': 1, 'repair': 1, 'nonpermissive': 1, 'permissive': 1, 'nearly': 1, 'semiconservative': 1, 'limited': 2, 'activated': 1, 'purine': 1, 'neurospora': 2, 'crassa': 2, 'former': 1, 'assessed': 1, 'absence': 2, 'purple': 1, 'pigment': 1, 'production': 3, 'tryptophan': 1, 'affected': 1, 'stimulated': 2, 'germination': 1, 'conidium': 1, 'correlated': 1, 'net': 3, 'nucleic': 1, 'mechanism': 2, 'dual': 1, 'action': 2, 'primary': 1, 'sarcoma': 1, 'connective': 1, 'adult': 2, 'blood': 1, 'vessel': 1, 'shorter': 1, 'latent': 1, 'ameboid': 1, 'marked': 2, 'proceeds': 1, 'markedly': 2, 'accelerated': 1, 'propagated': 1, 'long': 1, 'actively': 1, 'old': 1, 'method': 2, 'cultivation': 1, 'adapted': 1, 'pathological': 1, 'division': 1, 'nuclear': 2, 'change': 6, 'discernible': 1, 'staining': 1, 'applied': 1, 'verify': 1, 'observation': 4, 'unstained': 1, 'structure': 4, 'atypical': 1, 'mitoses': 1, 'seen': 4, 'rat': 2, 'kept': 1, 'body': 1, 'degree': 6, 'varies': 2, 'relatively': 1, 'narrow': 1, 'limit': 2, 'twenty': 1, 'fifty': 1, 'minute': 1, 'contrary': 1, 'hour': 2, 'amitotic': 1, 'tumor': 1, 'budding': 1, 'formation': 5, 'nucleus': 1, 'irregular': 1, 'noted': 2, 'mononuclear': 1, 'mitotic': 1, 'without': 1, 'cytoplasm': 1, 'potential': 2, 'energized': 1, 'mean': 1, 'ionic': 1, 'penetrants': 1, 'flux': 1, 'anion': 1, 'cation': 2, 'opposite': 1, 'direction': 1, 'stoichiometrically': 1, 'coupled': 1, 'outflow': 1, 'value': 5, 'distribution': 1, 'permanent': 1, 'mv': 2, 'minus': 1, 'penetrating': 2, 'deenergized': 1, 'following': 1, 'generation': 2, 'influx': 1, 'synthetic': 1, 'natural': 1, 'lactose': 1, 'collapsed': 1, 'maleimide': 1, 'favour': 1, 'conception': 1, 'generates': 1, 'driving': 1, 'force': 1, 'propranolol': 1, 'able': 1, 'titratable': 1, 'sonicated': 1, 'particle': 2, 'liposome': 1, 'salt': 2, 'solution': 1, 'fluorescence': 1, 'sulphonate': 1, 'suspension': 1, 'counteracted': 1, 'increasing': 1, 'concentration': 5, 'chloride': 1, 'aggregation': 1, 'phospholipid': 3, 'environment': 2, 'bound': 4, 'hydrophobic': 1, 'buffering': 1, 'hydrophilicity': 1, 'direct': 2, 'inverse': 1, 'sequence': 1, 'measure': 1, 'bathing': 1, 'abruptly': 1, 'reduced': 2, 'interpretation': 1, 'term': 2, 'calibration': 1, 'course': 2, 'entry': 1, 'adipocytes': 1, 'slowed': 1, 'reducing': 3, 'incubation': 2, 'bicarbonate': 1, 'buffer': 2, 'termination': 1, 'insulin': 1, 'includes': 1, 'disassociation': 1, 'step': 3, 'leading': 1, 'decelerated': 1, 'began': 1, 'complete': 3, 'transition': 3, 'steady': 1, 'approximated': 1, 'exponential': 1, 'process': 3, 'occurring': 1, 'comparison': 2, 'initiation': 1, 'measured': 2, 'curve': 3, 'virtually': 1, 'identical': 3, 'precise': 1, 'human': 2, 'hemoglobin': 1, 'determined': 3, 'automatic': 1, 'recording': 1, 'dpg': 1, 'inositol': 1, 'hexaphosphate': 1, 'ihp': 1, 'hydroxymethyl': 1, 'analyzed': 2, 'according': 1, 'adair': 1, 'scheme': 1, 'heat': 1, 'deltahi': 1, 'entropy': 2, 'deltasi': 1, 'oxygenation': 1, 'shape': 1, 'absent': 1, 'consequence': 2, 'depends': 1, 'behavior': 1, 'compensation': 2, 'contribution': 3, 'cdeltasi': 1, 'free': 2, 'energy': 1, 'compensated': 1, 'fourth': 1, 'accompanied': 2, 'major': 1, 'part': 2, 'nonuniformity': 1, 'attributable': 1, 'binding': 5, 'necessarily': 1, 'earlier': 2, 'idea': 1, 'wyman': 1, 'cooperative': 1, 'oxygenbinding': 1, 'liver': 2, 'homogeneity': 1, 'affinity': 4, 'chromatographic': 1, 'utilizing': 1, 'agarose': 1, 'isoelectric': 1, 'focusing': 1, 'revealed': 2, 'polyacrylamide': 2, 'gel': 4, 'electrophoresis': 3, 'demonstrated': 1, 'band': 1, 'fucosidase': 1, 'trace': 1, 'glycosidases': 1, 'analysis': 5, 'carbohydrate': 1, 'indicated': 1, 'filtration': 2, 'sepharose': 1, 'approximate': 1, 'yielded': 2, 'sodium': 2, 'dodecyl': 2, 'single': 3, 'optimum': 2, 'michaelis': 2, 'constant': 5, 'velocity': 1, 'mm': 2, 'little': 1, 'although': 3, 'dound': 1, 'monospecific': 1, 'crude': 1, 'fluid': 1, 'pure': 1, 'antigen': 1, 'patient': 1, 'died': 1, 'fucosidosis': 1, 'oxalacetate': 1, 'firmly': 1, 'cannof': 1, 'dissociation': 1, 'activation': 2, 'reductive': 1, 'titration': 2, 'reduction': 2, 'monitored': 1, 'bleaching': 1, 'chromophoric': 1, 'equivalent': 2, 'stoichiometry': 1, 'iron': 2, 'flavin': 1, 'content': 3, 'determination': 2, 'poised': 1, 'consistent': 2, 'moiety': 2, 'rather': 2, 'concurrent': 1, 'hydroquinone': 1, 'estimated': 1, 'adduct': 1, 'formed': 2, 'subtilisin': 1, 'bpn': 1, 'benzeneboronic': 1, 'diffraction': 1, 'resolution': 1, 'computed': 1, 'refined': 1, 'native': 1, 'covalent': 1, 'bond': 1, 'ogamma': 1, 'boron': 1, 'atom': 1, 'coordinated': 1, 'tetrahedrally': 1, 'additional': 2, 'boronic': 1, 'lying': 1, 'oxyanion': 1, 'hole': 2, 'leaving': 2, 'robertus': 1, 'kraut': 1, 'alden': 1, 'birktoft': 1, 'biochemistry': 2, 'postulated': 1, 'tetrahedral': 1, 'intermediate': 2, 'hydrolysis': 3, 'isosteric': 1, 'therefore': 2, 'considered': 1, 'good': 1, 'model': 2, 'koehler': 1, 'lienhard': 1, 'suggestion': 1, 'stabilization': 1, 'relative': 3, 'acyl': 1, 'hydrogen': 1, 'donation': 1, 'carbonyl': 1, 'side': 2, 'chain': 4, 'amido': 1, 'backbone': 1, 'cyclic': 1, 'dictyostelium': 2, 'discoideum': 2, 'amoeba': 2, 'induces': 1, 'extracellular': 1, 'phosphodiesterase': 1, 'induced': 1, 'starved': 1, 'appearance': 1, 'actinomycin': 1, 'daunomycin': 1, 'km': 1, 'uninduced': 1, 'since': 2, 'excretion': 1, 'inhibitory': 1, 'excluded': 3, 'lysates': 1, 'pyruvate': 1, 'oxidase': 1, 'composed': 1, 'soluble': 1, 'ferricytochrome': 1, 'oxidoreductase': 1, 'fraction': 2, 'lipid': 1, 'fractionation': 1, 'abut': 1, 'neutral': 1, 'ubiquinone': 1, 'menaquinone': 1, 'molar': 1, 'extraction': 1, 'aqueous': 1, 'acetone': 1, 'cereus': 1, 'phospholipase': 1, 'extracted': 2, 'practically': 1, 'phosphorus': 2, 'diglycerides': 1, 'remain': 1, 'dodecylamide': 1, 'restoration': 1, 'restores': 1, 'furthermore': 1, 'restore': 1, 'quinone': 1, 'photoinactivated': 1, 'reconstitution': 1, 'fact': 1, 'appear': 2, 'suppress': 1, 'mixture': 1, 'glycerophospholipids': 1, 'deacylated': 1, 'auxotrophic': 1, 'phosphatidylethanolamine': 1, 'methyltransferase': 1, 'phosphatidylmonomethylethanolamine': 1, 'dimethylethanolamine': 1, 'inos': 1, 'phosphatase': 1, 'constructed': 1, 'supplement': 1, 'adequate': 1, 'bizarre': 1, 'composition': 1, 'choice': 1, 'vary': 1, 'every': 1, 'cardiolipin': 1, 'maximum': 1, 'encountered': 1, 'zwitterionic': 1, 'expressed': 1, 'cent': 1, 'lecithin': 2, 'phosphatidyldimethylethanolamine': 1, 'anionic': 1, 'phosphatidylserine': 1, 'phosphatidylinositol': 1, 'despite': 1, 'wide': 1, 'proportion': 1, 'quantity': 1, 'remained': 1, 'existence': 1, 'internal': 1, 'maintenance': 1, 'fairly': 2, 'component': 2, 'charge': 1, 'fragment': 2, 'plasmin': 1, 'digest': 1, 'fibrinogen': 1, 'summation': 1, 'ultracentrifugation': 1, 'depending': 1, 'volume': 1, 'derived': 2, 'aalpha': 1, 'bbeta': 1, 'gamma': 2, 'differed': 1, 'extent': 3, 'degradation': 2, 'cleaved': 1, 'release': 1, 'beta': 3, 'alpha': 2, 'resistant': 1, 'digestion': 1, 'fibrin': 1, 'dimeric': 1, 'incorporating': 1, 'fluorescent': 1, 'label': 1, 'monodansyl': 1, 'cadaverine': 1, 'acceptor': 2, 'monomer': 2, 'generated': 2, 'unequivocally': 1, 'must': 1, 'recently': 1, 'proposed': 2, 'postulate': 1, 'incorrect': 1, 'alter': 1, 'detectably': 1, 'ranged': 1, 'stage': 1, 'degraded': 1, 'progressively': 2, 'brightly': 1, 'weakly': 1, 'segment': 1, 'atpase': 1, 'fragmented': 1, 'sarcoplasmic': 1, 'reticulum': 1, 'ikemoto': 1, 'biol': 1, 'chem': 1, 'denoted': 1, 'congruent': 1, 'dalton': 1, 'reversible': 1, 'parallelism': 1, 'sumida': 1, 'tonomura': 1, 'biochem': 1, 'dependence': 2, 'atp': 2, 'involvement': 1, 'low': 3, 'g': 2, 'permitting': 1, 'investigation': 1, 'phosphorylation': 1, 'dephosphorylation': 1, 'concomitantly': 1, 'e': 1, 'p': 2, 'calcium': 1, 'released': 1, 'rebound': 1, 'inhibits': 1, 'liberation': 1, 'pi': 1, 'hill': 1, 'plot': 1, 'tentatively': 1, 'gtp': 1, 'gpp': 1, 'nh': 1, 'plasma': 1, 'fat': 1, 'hydrolyzed': 1, 'pyrophosphohydrolases': 1, 'primarily': 1, 'phosphohydrolases': 1, 'guanine': 1, 'analogous': 1, 'spare': 1, 'taken': 1, 'inhibit': 1, 'intact': 2, 'irrespective': 1, 'unbound': 1, 'indicating': 2, 'siteis': 1, 'incapable': 1, 'degrading': 1, 'gdp': 1, 'competitively': 1, 'mum': 2, 'adenylate': 1, 'cyclase': 1, 'sulfhydryl': 1, 'agent': 1, 'substantial': 1, 'regenerating': 1, 'differentially': 1, 'susceptible': 1, 'terminal': 1, 'potency': 1, 'activator': 1, 'thyroglobulin': 1, 'guinea': 1, 'pig': 1, 'na': 1, 'alkylation': 1, 'mammalian': 1, 'polypeptide': 1, 'accounted': 1, 'guanidine': 1, 'hcl': 1, 'gave': 1, 'large': 2, 'satisfactory': 2, 'whole': 1, 'lysine': 1, 'glutamic': 1, 'iodine': 1, 'reveal': 1, 'preference': 1, 'oligoribonucleotides': 1, 'indeed': 1, 'inversely': 1, 'proportional': 1, 'length': 1, 'attack': 1, 'polynucleotides': 1, 'dnase': 1, 'monovalent': 1, 'polynucleotide': 1, 'phosphorylase': 1, 'display': 1, 'thermostability': 1, 'point': 1, 'distinction': 1, 'outer': 1, 'layer': 1, 'peptidoglycan': 1, 'act': 1, 'barrier': 1, 'sieve': 1, 'penetration': 1, 'uncharged': 1, 'saccharide': 1, 'decad': 1, 'nakae': 1, 'nikaido': 1, 'fed': 1, 'proc': 1, 'studying': 1, 'whose': 1, 'destroyed': 1, 'lysozyme': 1, 'penicillin': 1, 'plasmolyzed': 1, 'allowed': 1, 'stachyose': 1, 'acted': 1, 'u': 1, 'conclude': 1, 'set': 1, 'hydrophilic': 1, 'exclusion': 1, 'leakiness': 1, 'decreased': 1, 'producing': 1, 'extremely': 1, 'lipopolysaccharide': 1, 'trypsin': 1, 'followed': 1, 'heating': 1, 'cooling': 1, 'feel': 1, 'hypothesis': 1, 'resealing': 1, 'ruptured': 1, 'isolation': 1, 'often': 1, 'incomplete': 1, 'crack': 1, 'responsible': 1, 'ionization': 1, 'residue': 3, 'beenobtained': 1, 'magnetic': 1, 'resonance': 1, 'imidazole': 1, 'proton': 1, 'thermodynamic': 1, 'parameter': 1, 'exposed': 1, 'solvent': 1, 'restricted': 1, 'measurement': 1, 'inflection': 1, 'negative': 1, 'givine': 1, 'rise': 1, 'buried': 1, 'bovine': 1, 'testicular': 1, 'hyaluronidase': 1, 'radioactively': 1, 'labeled': 1, 'oligomers': 1, 'hyalobiuronic': 1, 'transglycosylation': 1, 'transfer': 1, 'glycosyl': 1, 'retention': 1, 'configuration': 1, 'detailed': 1, 'examination': 1, 'trimer': 1, 'tetramer': 1, 'polymer': 1, 'consists': 1, 'subsites': 1, 'hyalobiuronate': 1, 'terminology': 1, 'schechter': 1, 'berger': 1, 'terminus': 1, 'right': 1, 'subsite': 1, 'proposal': 1, 'mechanistic': 1, 'implication': 1, 'actin': 1, 'polymerizes': 1, 'filament': 1, 'kcl': 1, 'judged': 1, 'optical': 1, 'nm': 1, 'supramolecular': 1, 'assembly': 1, 'optimal': 1, 'striated': 1, 'muscle': 1, 'resembles': 1, 'assemble': 1, 'bundle': 1, 'diameter': 1, 'reconstituted': 1, 'resemble': 1, 'situ': 1, 'many': 1, 'cytochrome': 1, 'structurally': 1, 'topologically': 1, 'globin': 1, 'fold': 1, 'superimposed': 1, 'heme': 1, 'inclined': 1, 'proximal': 1, 'adjacent': 1, 'helix': 1, 'sixth': 1, 'ligand': 1, 'distal': 1, 'coincident': 1, 'changeable': 1, 'rotated': 1, 'energetically': 1, 'conservative': 1, 'directly': 1, 'inserted': 1, 'lining': 1, 'pocket': 1, 'roughly': 1, 'missing': 1, 'carboxy': 1, 'end': 1, 'correspond': 1, 'would': 1, 'probable': 1, 'similarity': 1, 'divergent': 1, 'evolution': 1, 'primordial': 1, 'convergence': 1, 'functionally': 1, 'chromatography': 1, 'column': 1, 'isolate': 1, 'lectin': 1, 'concanavalin': 1, 'favin': 1, 'phytohemagglutinin': 1, 'wheat': 1, 'germ': 1, 'agglutinin': 1, 'limulus': 1, 'hemagglutinin': 1, 'specificity': 1, 'california': 1, 'white': 1, 'bean': 1, 'idaho': 1, 'red': 1, 'pea': 1, 'mapping': 1, 'lymphocyte': 1, 'sperm': 1, 'particularly': 1, 'striking': 1, 'mouse': 1, 'spleen': 1, 'accord': 1, 'electrophoretic': 1, 'radiolabeled': 1, 'receptor': 1, 'adsorbent': 1, 'posse': 1, 'strikingly': 1}\n"
     ]
    }
   ],
   "source": [
    "print(doc_freq(bow_collections[\"medline\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "medline = tf_idf(bow_collections[\"medline\"],idf(bow_collections[\"medline\"]))\n",
    "wsj = tf_idf(bow_collections[\"wsj\"], idf(bow_collections[\"wsj\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1684416024776631\n",
      "0.14802615755488274\n",
      "0.03812192439111871\n"
     ]
    }
   ],
   "source": [
    "#print(avg_sim(tf_idf(bow_collections[\"medline\"],idf(bow_collections[\"wsj\"]))))\n",
    "#print(avg_sim(tf_idf(bow_collections[\"wsj\"], idf(bow_collections[\"wsj\"]))))\n",
    "#print(avg_sim(tf_idf(bow_collections[\"medline\"], idf(bow_collections[\"medline\"]))))\n",
    "\n",
    "print(avg_sim(medline,medline))\n",
    "print(avg_sim(wsj,wsj))\n",
    "print(avg_sim(medline,wsj))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c). Expand the document representations by adding **synonyms** and **hypernyms** for each **noun** in the document.  For example, 2 occurrences of the word *tiger* should add 2 occurrences of each of the following **lemma_names** found in the WordNet hypernym hierarchy above *tiger*:\n",
    "* \\['tiger', 'Panthera_tigris'\\]\n",
    "* \\['big_cat', 'cat'\\]\n",
    "* \\['feline', 'felid'\\]\n",
    "* \\['carnivore'\\]\n",
    "* \\['placental', 'placental_mammal', 'eutherian', 'eutherian_mammal'\\]\n",
    "* \\['mammal', 'mammalian'\\]\n",
    "* \\['vertebrate', 'craniate'\\]\n",
    "* \\['chordate'\\]\n",
    "* \\['animal', 'animate_being', 'beast', 'brute', 'creature', 'fauna'\\]\n",
    "* \\['organism', 'being'\\]\n",
    "* \\['living_thing', 'animate_thing'\\]\n",
    "* \\['whole', 'unit'\\]\n",
    "* \\['object', 'physical_object'\\]\n",
    "* \\['physical_entity'\\]\n",
    "* \\['entity'\\]\n",
    "\n",
    "Recompute the similarities calculated in part b).  Discuss your results. \\[9 marks\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: Supervised Methods for WSD (25 marks)\n",
    "The objective of this question is to build and evaluate a word sense disambiguation (WSD) system for words with multiple senses.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a).  For each word occurring in the medline corpus (defined above), **write code** to find how many senses it has according to WordNet.  Print a list of the 10 most frequently occurring words with 2 senses (in this corpus). \\[4 marks\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bow_collections' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-409d9a941590>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#get most common word(method is there)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mraw_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbow_collections\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'medline'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0msense_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bow_collections' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import wordnet_ic as wn_ic\n",
    "from collections import Counter\n",
    "\n",
    "#convert medline to sentence\n",
    "#convert sentences into tokens \n",
    "#initilize empty list\n",
    "#add all sentences to that list\n",
    "#for every word in the word list, check if the word has two sentces wn.synsets(word)\n",
    "#if it has two, add to the empty list\n",
    "#get most common word(method is there)\n",
    "\n",
    "raw_sentences = bow_collections['medline']\n",
    "sense_list=[]\n",
    "\n",
    "for sentence in raw_sentences: \n",
    "    for word in sentence:\n",
    "        if len(wn.synsets(word)) == 2:\n",
    "            sense_list.append(word)\n",
    "counter = Counter(sense_list)\n",
    "most_occur = counter.most_common(10)\n",
    "print(most_occur)\n",
    "            \n",
    "#print(bow_collections[\"medline\"])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b). A *supervised* WSD algorithm derives model(s) from *sense-annotated corpus data* in order to predict senses of ambiguous words in un-annotated data.  Using the entire document as context, **implement** a supervised word sense disambiguation algorithm to determine the most likely sense of each occurrence of the 3 most frequently occuring words identified in part a). \\[8 marks\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator \n",
    "\n",
    "def max_sim(word, contextlemmas,pos=wn.NOUN):\n",
    "    synsets=wn.synsets(word,pos)\n",
    "    scores=[]\n",
    "    for synset in synsets:\n",
    "        total=0\n",
    "        for lemma in contextlemmas:\n",
    "            sofar=0\n",
    "        for synsetB in wn.synsets(lemma,pos):\n",
    "            sim=wn.path_similarity(synset,synsetB)\n",
    "            if sim>sofar:\n",
    "                sofar=sim\n",
    "            total+=sofar\n",
    "        scores.append((synset.definition(),total))\n",
    "    sortedscores=sorted(scores,key=operator.itemgetter(1), reverse=True)\n",
    "    #print(sortedscores)\n",
    "    return sortedscores[0]\n",
    "\n",
    "#we have: list of documents (medline corpus)\n",
    "#we want: list of strings (documents in medline corpus as string)\n",
    "#initialize a new list, which is where the string of document will go\n",
    "def makestring(corpus, pos):\n",
    "    docs_as_strings = []\n",
    "    for document in corpus:\n",
    "        docs_as_strings.append(' '.join(document))\n",
    "        #docs_as_strings[raw_sentences]\n",
    "    return docs_as_strings\n",
    "\n",
    "#takes the word in the corpus, takes its feature(noun,verb, etc) \n",
    "#takes makes string method and applies to corpus\n",
    "#loop through the corpus , for every sentence: if a word x is in the sentence\n",
    "#if the word is in the sentence-->\n",
    "# make a new list, and append the word to the list\n",
    "#we want to store the sense of the word in th lsit so we call max_sim\n",
    "#\n",
    "\n",
    "def sense(word, corpus, pos):\n",
    "    x = makestring(corpus, pos)\n",
    "    senses_list = []\n",
    "    for sentence in x:\n",
    "         if word in sentence:\n",
    "            senses_list.append(max_sim(word, sentence, pos))\n",
    "    return senses_list\n",
    "sense(\"temperature\", raw_sentences, wn.NOUN)          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c). Evaluate the performance of your WSD system.  How accurate is it for each of the 3 words? **Comment** on the strengths and weaknesses of your WSD system.\\[8 marks\\] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#word sense disambiguation \n",
    "in this corpus, the 10 most freuquent words are:\n",
    "    [('membrane', 10), ('temperature', 7), ('molecular', 7), \n",
    "     ('uptake', 6), ('data', 6), ('molecule', 6), ('may', 5), \n",
    "     ('amino', 5), ('phenomenon', 5), ('ratio', 4)]\n",
    "\n",
    "The most frequent word in this corpus is membrane, \n",
    "the top 10 most frequent words in this corpus all carry a predominant sense. \n",
    "There are two senses to the word membrane, the biological definition \n",
    "\" microscopic double layer of lipids and proteins forming the boundary of cells or organelles.\",\n",
    "and a more generic, definition that is not necesarrily medical.\n",
    "the WSD above shows that the sense of the word membrane in this corpus would be defined as \n",
    "\"a thin pliable sheet of material\".\n",
    "this proves that the our WSD is weak and not accurate, since it gave a sense to word membrane that is not bioological or medical,\n",
    "regardless of the large distribution of word senses skewed towards the bioloical domain.\n",
    "\n",
    "the second most frequent word is 'temperature':\n",
    "which has a predominant sense, since most workds have a highly skewed sense distribution. however, regardless of the predominant meaning of the \n",
    "word temperature which is: \"the degree or intensity of heat present in a substance or object, especially as expressed according to a comparative \n",
    "scale and shown by a thermometer or perceived by touch.\"\n",
    "the WSD was accurate predicting the sense of this word as :\"'the degree of hotness or coldness of a body or environment\", which is the less dominant,\n",
    "but more medical. \n",
    "\n",
    "uptake, has two senses:\n",
    "the more medical sense would be \"the taking in or absorption of a substance by a living organism or bodily organ.\"\n",
    "however the WSD was weak in its accuracy and chose the word sense that is more generic and non-medical:\n",
    "\"a process of taking up or using up or consuming\"\n",
    "\n",
    "\n",
    "Data: has two word senses, one being a more predominant sense towards scientific industry and the other is part of a\n",
    "    philosophical domain:\n",
    "the WSD was accurate in choosing the sense that was scientific, and hence more medical, relative to the philosophical sense.\n",
    "\"a collection of facts from which conclusions may be drawn\"\n",
    "\n",
    "molecule: two word senses\n",
    "the WSD was accurate in picking the correct scientific sense of the word\n",
    "'(physics and chemistry) the simplest structural unit of an element or compound'\n",
    "\n",
    "\n",
    "may: a predominant sense, \n",
    "meaning: expressing possibility.\n",
    "less dominant, but possibly medical sense: \"expressing possibility.\"\n",
    "the WSD was inaccurate and chose the sense which is non-medical\n",
    "\"the month following April and preceding June'\"\n",
    "\n",
    "amino: a mesynemous word, meaning it has only one meaning.\n",
    "WSD was accurate:\n",
    "'the radical -NH2\n",
    "\n",
    "phenomenon: WSD inaccurate,\n",
    "    two word senses, and the WSD chose philosophical as oppose to medical term.\n",
    "  \"any state or process known through the senses rather than by intuition or reasoning'\"  \n",
    "correct sense: \"a fact or situation that is observed to exist or happen, especially one whose cause or explanation is in question.\"\n",
    "    \n",
    "    \n",
    "ratio: WSD accurate:\n",
    "mesynemous word, singular meaning that is medical:\n",
    "\"\"the relative magnitudes of two quantities\n",
    "    \n",
    "    \n",
    "calculating accuracy of WSD based on the word senses chosen:\n",
    " accuracy = accurate senses/total number of frequent senses\n",
    " accuracy = 4/9 \n",
    "\n",
    " \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) How could you extend or improve your WSD system?  You are **not** expected to code any of these extensions or improvements, but your answer should give sufficient details to make it clear how they might be carried out in practice. \\[5 marks\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-the WSD system could improve by grouping phrases together, as bigrams, trigrams, etc.\n",
    "-this could be done by checking the frequency of occurence of certain phrases togethers, (more than is usualy by chance)\n",
    "-thesephrases will tell us a lot about the sense of the document. this will increase accuracy, since \n",
    "co-occurence of words together might tell us more about the sense of the document.\n",
    "-however this might be an issue when it comes to weighing the n-grams, because phrases would weig more than uni-grams(words that appear individually).\n",
    "-an approach to fix this problem will be using the tf-idf SCORE\n",
    "\n",
    "-the WSD could also be improved by performing disambiguation by comparing overlap between dictionary definitions,\n",
    "-the WSD could be further improved by using the simplified lesk method, which looks\n",
    "at the overlap between the definitions of the senses of the ambigious word and the word we re considering.\n",
    "-the WSD could improve by using the naive bayes classifier instead, where the classifier can decide which class or 'sense'\n",
    "the word belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the code below to verify that the length of your submission does not exceed 3000 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission length is 3962\n"
     ]
    }
   ],
   "source": [
    "##This code will word count all of the markdown cells in the notebook saved at filepath\n",
    "##Running it before providing any answers shows that the questions have a word count of 1202\n",
    "\n",
    "import io\n",
    "from nbformat import current\n",
    "\n",
    "filepath=\"a1.ipynb\"\n",
    "question_count=1202\n",
    "\n",
    "with io.open(filepath, 'r', encoding='utf-8') as f:\n",
    "    nb = current.read(f, 'json')\n",
    "\n",
    "word_count = 0\n",
    "for cell in nb.worksheets[0].cells:\n",
    "    if cell.cell_type == \"markdown\":\n",
    "        word_count += len(cell['source'].replace('#', '').lstrip().split(' '))\n",
    "print(\"Submission length is {}\".format(word_count-question_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
